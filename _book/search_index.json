[["index.html", "Introduction to R for GIS Chapter 1 Background, Goals and Data 1.1 R, GISc, and Goals for this book 1.2 Exploratory Data Analysis 1.3 Software and Data", " Introduction to R for GIS Jerry Davis, SFSU Institute for Geographic Information Science 2023-04-06 Chapter 1 Background, Goals and Data 1.1 R, GISc, and Goals for this book While the R language was originally developed to do statistical analysis, and create graphics in support of a focus on exploratory data analysis, it has been extended in many directions, one of which is geospatial. This course is designed to provide a brief introduction to the R language, especially as enhanced for clarity by methods in the “tidyverse”, continue that philosophical approach into visualization methods, and then look at how to put our data on map. This book focuses on methods for a 2-day course introducing R for GIS, and is a subset of a longer Introduction to Environmental Data Science book published by CRC Press, which also gets into additional graphics, statistical modeling, imagery classification, time series, and other R methods. A free online version hosted at bookdown.org: https://bookdown.org/igisc/EnvDataSci/, which I use for a semester-long course. We obviously won’t be covering that much material, but I’ve provided suggestions of exploring further by referencing that book. R turns out to be a very accessible entry into data science. In general, data science can be seen as being the intersection of math and statistics, computer science/IT, and some research domain, and in this case it’s environmental. GISc plays an important part in making this work. 1.2 Exploratory Data Analysis Just as exploration is a part of what National Geographic has long covered, it’s an important part of geographic and environmental science research. Exploratory data analysis is exploration applied to data, and has grown as an alternative approach to traditional statistical analysis. This basic approach perhaps dates back to the work of Thomas Bayes in the eighteenth century, but Tukey (1962) may have best articulated the basic goals of this approach in defining the “data analysis” methods he was promoting: “Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.” Some years later Tukey (1977) followed up with Exploratory Data Analysis. Exploratory data analysis (EDA) is an approach to analyzing data via summaries and graphics. The key word is exploratory, and while one might view this in contrast to confirmatory statistics, in fact they are highly complementary. The objectives of EDA include (a) suggesting hypotheses; (b) assessing assumptions on which inferences will be based; (c) selecting appropriate statistical tools; and (d) guiding further data collection. This philosophy led to the development of S at Bell Labs (led by John Chambers, 1976), then to R. 1.3 Software and Data First, we’re going to use the R language, designed for statistical computing and graphics. It’s not the only way to do data analysis – Python is another important data science language – but R with its statistical foundation is an important language for academic research, especially in the environmental sciences. ## [1] &quot;This book was produced in RStudio using R version 4.2.3 (2023-03-15 ucrt)&quot; For a start, you’ll need to have R and RStudio installed on the computer you’ll be using. If you’re working in our lab, R and RStudio are already installed, and just need to install packages that extend the software, which is very easy to do in R, and everything can be done within RStudio. If and only if you are working on your own computer, I’d recommend doing a clean uninstall of previous versions of R before installing a new version. In Windows, you can find the unins000.exe file in the version folder within C:\\Program Files\\R. You’ll find a link to install the latest version “for the first time” at CRAN (search “R CRAN”). In RStudio, you can then install the packages needed. You’ll want to install them when you first need them, which will typically be when you first see a library() call in the code, or possibly when a function is prefaced with the package name, something like dplyr::select(), or maybe when R raises an error that it can’t find a function you’ve called or that the package isn’t installed. One of the earliest we’ll need is the suite of packages in the “tidyverse” (Wickham and Grolemund (2016)), which includes some of the ones listed above: ggplot2, dplyr, stringr, and tidyr. You can install these individually, or all at once with: `install.packages(&quot;tidyverse&quot;)` This is usually done from the console in RStudio and not included in an R script or markdown document, since you don’t want to be installing the package over and over again. You can also respond to a prompt from RStudio when it detects a package called in a script you open that you don’t have installed. From time to time, you’ll want to update your installed packages, and that usually happens when something doesn’t work and maybe the dependencies of one package on another gets broken with a change in a package. Fortunately, in the R world, especially at the main repository at CRAN, there’s a lot of effort put into making sure packages work together, so usually there are no surprises if you’re using the most current versions. Note that there can be exceptions to this, and occasionally new package versions will create problems with other packages due to inter-package dependencies and the introduction of functions with names that duplicate other packages. The packages installed for this book were current as of that version of R, but new package versions may occasionally introduce errors. Once a package like dplyr is installed, you can access all of its functions and data by adding a library call, like … library(dplyr) … which you will want to include in your code, or to provide access to multiple libraries in the tidyverse, you can use library(tidyverse). Alternatively, if you’re only using maybe one function out of an installed package, you can call that function with the :: separator, like dplyr::select(). This method has another advantage in avoiding problems with duplicate names – and for instance we’ll generally call dplyr::select() this way. 1.3.1 Data We’ll be using data from various sources, including data on CRAN like the code packages above which you install the same way – so use install.packages(\"palmerpenguins\"). We’ve also created a repository on GitHub that includes data we’ve developed in the Institute for Geographic Information Science (iGISc) at SFSU, and you’ll need to install that package a slightly different way. GitHub packages require a bit more work on the user’s part since we need to first install remotes1, then use that to install the GitHub data package: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;iGISc/igisci&quot;) Then you can access it just like other built-in data by including: library(igisci) To see what’s in it, you’ll see the various datasets listed in: data(package=&quot;igisci&quot;) For instance, Figure 1.1 is a map of California counties using the CA_counties sf feature data. We’ll be looking at the sf (Simple Features) package later in the Spatial section of the book, but seeing library(sf), this is one place where you’d need to have installed another package, with install.packages(\"sf\"). library(tidyverse); library(igisci); library(sf) ggplot(data=CA_counties) + geom_sf() FIGURE 1.1: California counties simple features data in igisci package The package datasets can be used directly as sf data or data frames. And similarly to functions, you can access the (previously installed) data set by prefacing with igisci:: this way, without having to load the library. This might be useful in a one-off operation: mean(igisci::sierraFeb$LATITUDE) ## [1] 38.3192 Raw data such as .csv files can also be read from the extdata folder that is installed on your computer when you install the package, using code such as: csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI/TRI_1987_BaySites.csv&quot;, package=&quot;igisci&quot;) TRI87 &lt;- read_csv(csvPath) or something similar for shapefiles, such as: shpPath &lt;- system.file(&quot;extdata&quot;,&quot;marbles/trails.shp&quot;, package=&quot;igisci&quot;) trails &lt;- st_read(shpPath) And we’ll find that including most of the above arcanity in a function will help. We’ll look at functions later, but here’s a function that we’ll use a lot for setting up reading data from the extdata folder: ex &lt;- function(dta){system.file(&quot;extdata&quot;,dta,package=&quot;igisci&quot;)} And this ex()function is needed so often that it’s installed in the igisci package, so if you have library(igisci) in effect, you can just use it like this: trails &lt;- st_read(ex(&quot;marbles/trails.shp&quot;)) But how do we see what’s in the extdata folder? We can’t use the data() function, so we would have to dig for the folder where the igisci package gets installed, which is buried pretty deeply in your user profile. So I wrote another function exfiles() that creates a data frame showing all of the files and the paths to use. In RStudio you could access it with View(exfiles()) or we could use a datatable (you’ll need to have installed “DT”). You can use the path using the ex() function with any function that needs it to read data, like read.csv(ex('CA/CA_ClimateNormals.csv')), or just enter that ex() call in the console like ex('CA/CA_ClimateNormals.csv') to display where on your computer the installed data reside. DT::datatable(exfiles(), options=list(scrollX=T), rownames=F) References "],["introduction.html", "Chapter 2 Introduction to R 2.1 Data Objects 2.2 Functions 2.3 Expressions and Statements 2.4 Data Classes 2.5 Rectangular Data 2.6 Data Structures in R 2.7 Accessors and Subsetting 2.8 Programming scripts in RStudio 2.9 RStudio projects 2.10 Exercises: Introduction to R", " Chapter 2 Introduction to R This section lays the foundation for exploratory data analysis using the R language and packages especially within the tidyverse. This foundation progresses through: Introduction : An introduction to the R language Abstraction : Exploration of data via reorganization using dplyr and other packages in the tidyverse (Chapter 3) Visualization : Adding visual tools to enhance our data exploration (Chapter 4) Transformation : Reorganizing our data with pivots and data joins (Chapter ??) In this chapter we’ll introduce the R language, using RStudio to explore its basic data types, structures, functions and programming methods in base R. We’re assuming you’re either new to R or need a refresher. Later chapters will add packages that extend what you can do with base R for data abstraction, transformation, and visualization, then explore the spatial world, statistical models, and time series applied to environmental research. The following code illustrates a few of the methods we’ll explore in this chapter: temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3) elev &lt;- c(52, 394, 510, 564, 725) lat &lt;- c(39.52, 38.91, 37.97, 38.70, 39.09) elevft &lt;- round(elev / 0.3048) deg &lt;- as.integer(lat) min &lt;- as.integer((lat-deg) * 60) sec &lt;- round((lat-deg-min/60)*3600) sierradata &lt;- cbind(temp, elev, elevft, lat, deg, min, sec) mydata &lt;- as.data.frame(sierradata) mydata ## temp elev elevft lat deg min sec ## 1 10.7 52 171 39.52 39 31 12 ## 2 9.7 394 1293 38.91 38 54 36 ## 3 7.7 510 1673 37.97 37 58 12 ## 4 9.2 564 1850 38.70 38 42 0 ## 5 7.3 725 2379 39.09 39 5 24 RStudio If you’re new to RStudio, or would like to learn more about using it, there are plenty of resources you can access to learn more about using it. As with many of the major packages we’ll explore, there’s even a cheat sheet: https://www.rstudio.com/resources/cheatsheets/. Have a look at this cheat sheet while you have RStudio running, and use it to learn about some of its different components: The Console, where you’ll enter short lines of code, install packages, and get help on functions. Messages created from running code will also be displayed here. There are other tabs in this area (e.g. Terminal, R Markdown) we may explore a bit, but mostly we’ll use the console. The Source Editor, where you’ll write full R scripts and R Markdown documents. You should get used to writing complete scripts and R Markdown documents as we go through the book. Various Tab Panes such as the Environment pane, where you can explore what scalars and more complex objects contain. The Plots pane in the lower right for static plots (graphs and maps that aren’t interactive), which also lets you see a listing of Files, or View interactive maps and maps. 2.1 Data Objects As with all programming languages, R works with data and since it’s an object-oriented language, these are data objects. Data objects can range from the most basic type – the scalar which holds one value, like a number or text – to everything from an array of values to spatial data for mapping or a time series of data. 2.1.1 Scalars and assignment We’ll be looking at a variety of types of data objects, but scalars are the most basic type, holding individual values, so we’ll start with it. Every computer language, like in math, stores values by assigning them constants or results of expressions. These are often called “variables,” but we’ll be using that name to refer to a column of data stored in a data frame, which we’ll look at later in this chapter. R uses a lot of objects, and not all are data objects; we’ll also create functions ??, a type of object that does something (runs the function code you’ve defined for it) with what you provide it. To create a scalar (or other data objects), we’ll use the most common type of statement, the assignment statement, that takes an expression and assigns it to a new data object that we’ll name. The class of that data object is determined by the class of the expression provided, and that expression might be something as simple as a constant like a number or a character string of text. Here’s an example of a very basic assignment statement that assigns the value of a constant 5 to a new scalar x: x &lt;- 5 Note that this uses the assignment operator &lt;- that is standard for R. You can also use = as most languages do (and I sometimes do), but we’ll use = for other types of assignments. All object names must start with a letter, have no spaces, and must not use any names that are built into the R language or used in package libraries, such as reserved words like for or function names like log. Object names are case-sensitive (which you’ll probably discover at some point by typing in something wrong and getting an error). x &lt;- 5 y &lt;- 8 Longitude &lt;- -122.4 Latitude &lt;- 37.8 my_name &lt;- &quot;Inigo Montoya&quot; To check the value of a data object, you can just enter the name in the console, or even in a script or code chunk. x ## [1] 5 y ## [1] 8 Longitude ## [1] -122.4 Latitude ## [1] 37.8 my_name ## [1] &quot;Inigo Montoya&quot; This is counter to the way printing out values commonly works in other programming languages, and you will need to know how this method works as well because you will want to use your code to develop tools that accomplish things, and there are also limitations to what you can see by just naming objects. To see the values of objects in programming mode, you can also use the print() function (but we rarely do); or to concatenate character string output, use paste() or paste0. print(x) paste0(&quot;My name is &quot;, my_name, &quot;. You killed my father. Prepare to die.&quot;) Numbers concatenated with character strings are converted to characters. paste0(paste(&quot;The Ultimate Answer to Life&quot;, &quot;The Universe&quot;, &quot;and Everything is ... &quot;, sep=&quot;, &quot;),42,&quot;!&quot;) paste(&quot;The location is latitude&quot;, Latitude, &quot;longitude&quot;, Longitude) ## [1] &quot;The location is latitude 37.8 longitude -122.4&quot; Review the code above and what it produces. What do you think the difference is between paste() and paste0()? We’ll use paste0() a lot in this book to deal with long file paths which create problems for the printed/pdf version of this book, basically extending into the margins. Breaking the path into multiple strings and then combining them with paste0() is one way to handle them. For instance, in the Imagery and Classification Models chapter, the Sentinel2 imagery is provided in a very long file path. So here’s how we use paste0() to recombine after breaking up the path, and we then take it one more step and build out the full path to the 20 m imagery subset. imgFolder &lt;- paste0(&quot;S2A_MSIL2A_20210628T184921_N0300_R113_T10TGK_20210628T230915.&quot;, &quot;SAFE/GRANULE/L2A_T10TGK_A031427_20210628T185628&quot;) img20mFolder &lt;- paste0(&quot;~/sentinel2/&quot;,imgFolder,&quot;/IMG_DATA/R20m&quot;) 2.2 Functions Just as in regular mathematics, R makes a lot of use of functions that accept an input and create an output: log10(100) log(exp(5)) cos(pi) sin(90 * pi/180) But functions can be much more than numerical ones, and R functions can return a lot of different data objects. You’ll find that most of your work will involve functions, from those in base R to a wide variety in packages you’ll be adding. You will likely have already used the install.packages() and library() functions that add in an array of other functions. Later in this chapter, we’ll also learn how to write our own functions, a capability that is easy to accomplish and also gives you a sense of what developing your own package might be like. Arithmetic operators There are, of course, all the normal arithmetic operators (that are actually functions) like plus + and minus - or the key-stroke approximations of multiply * and divide / operators. You’re probably familiar with these approximations from using equations in Excel if not in some other programming language you may have learned. These operators look a bit different from how they’d look when creating a nicely formatted equation. For example, \\(\\frac{NIR - R}{NIR + R}\\) instead has to look like (NIR-R)/(NIR+R). Similarly * must be used to multiply; there’s no implied multiplication that we expect in a math equation like \\(x(2+y)\\), which would need to be written x*(2+y). In contrast to those four well-known operators, the symbol used to exponentiate – raise to a power – varies among programming languages. R uses either ** or ^ so the the Pythagorean theorem \\(c^2=a^2+b^2\\) might be written c**2 = a**2 + b**2 or c^2 = a^2 + b^2 except for the fact that it wouldn’t make sense as a statement to R. Why? And how would you write an R statement that assigns the variable c an expression derived from the Pythagorean theorem? (And don’t use any new functions from a Google search – from deep math memory, how do you do \\(\\sqrt{x}\\) using an exponent?) a &lt;- 2; b &lt;- 3 c &lt;- (a^2 + b^2)^0.5 c ## [1] 3.605551 It’s time to talk more about expressions and statements. 2.3 Expressions and Statements The concepts of expressions and statements are very important to understand in any programming language. An expression in R (or any programming language) has a value just like an object has a value. An expression will commonly combine data objects and functions to be evaluated to derive the value of the expression. Here are some examples of expressions: 5 x x*2 sin(x) (a^2 + b^2)^0.5 (-b+sqrt(b**2-4*a*c))/2*a paste(&quot;My name is&quot;, aname) Note that some of those expressions used previously assigned objects – x, a, b, c, aname. An expression can be entered in the console to display its current value, and this is commonly done in R for objects of many types and complexity. cos(pi) ## [1] -1 Nile ## Time Series: ## Start = 1871 ## End = 1970 ## Frequency = 1 ## [1] 1120 1160 963 1210 1160 1160 813 1230 1370 1140 995 935 1110 994 1020 ## [16] 960 1180 799 958 1140 1100 1210 1150 1250 1260 1220 1030 1100 774 840 ## [31] 874 694 940 833 701 916 692 1020 1050 969 831 726 456 824 702 ## [46] 1120 1100 832 764 821 768 845 864 862 698 845 744 796 1040 759 ## [61] 781 865 845 944 984 897 822 1010 771 676 649 846 812 742 801 ## [76] 1040 860 874 848 890 744 749 838 1050 918 986 797 923 975 815 ## [91] 1020 906 901 1170 912 746 919 718 714 740 Whoa, what was that? We entered the expression Nile and got a bunch of stuff! Nile is a type of data object called a time series that we’ll be looking at much later, and since it’s in the built-in data in base R, just entering its name will display it. And since time series are also vectors which are like entire columns, rows, or variables of data, we can vectorize it (apply mathematical operations and functions element-wise) in an expression: Nile * 2 ## Time Series: ## Start = 1871 ## End = 1970 ## Frequency = 1 ## [1] 2240 2320 1926 2420 2320 2320 1626 2460 2740 2280 1990 1870 2220 1988 2040 ## [16] 1920 2360 1598 1916 2280 2200 2420 2300 2500 2520 2440 2060 2200 1548 1680 ## [31] 1748 1388 1880 1666 1402 1832 1384 2040 2100 1938 1662 1452 912 1648 1404 ## [46] 2240 2200 1664 1528 1642 1536 1690 1728 1724 1396 1690 1488 1592 2080 1518 ## [61] 1562 1730 1690 1888 1968 1794 1644 2020 1542 1352 1298 1692 1624 1484 1602 ## [76] 2080 1720 1748 1696 1780 1488 1498 1676 2100 1836 1972 1594 1846 1950 1630 ## [91] 2040 1812 1802 2340 1824 1492 1838 1436 1428 1480 More on that later, but we’ll start using vectors here and there. Back to expressions and statements: A statement in R does something. It represents a directive we’re assigning to the computer, or maybe the environment we’re running on the computer (like RStudio, which then runs R). A simple print() statement seems a lot like what we just did when we entered an expression in the console, but recognize that it does something: print(&quot;Hello, World&quot;) ## [1] &quot;Hello, World&quot; Which is the same as just typing \"Hello, World\", but either way we write it, it does something. Statements in R are usually put on one line, but you can use a semicolon to have multiple statements on one line, if desired: x &lt;- 5; print(x); print(x**2); x; x^0.5 ## [1] 5 ## [1] 25 ## [1] 5 ## [1] 2.236068 Many (perhaps most) statements don’t actually display anything. For instance: x &lt;- 5 doesn’t display anything, but it does assign the constant 5 to the object x, so it simply does something. It’s an assignment statement, easily the most common type of statement that we’ll use in R, and uses that special assignment operator &lt;- . Most languages just use = which the designers of R didn’t want to use, to avoid confusing it with the equal sign meaning “is equal to”. An assignment statement assigns an expression to a object. If that object already exists, it is reused with the new value. For instance it’s completely legit (and commonly done in coding) to update the object in an assignment statement. This is very common when using a counter scalar: i = i + 1 You’re simply updating the index object with the next value. This also illustrates why it’s not an equation: i=i+1 doesn’t work as an equation (unless i is actually \\(\\infty\\) but that’s just really weird.) And c**2 = a**2 + b**2 doesn’t make sense as an R statement because c**2 isn’t an object to be created. The ** part is interpreted as raise to a power. What is to the left of the assignment operator = must be an object to be assigned the value of the expression. 2.4 Data Classes Scalars, constants, vectors, and other data objects in R have data classes. Common types are numeric and character, but we’ll also see some special types like Date. x &lt;- 5 class(x) ## [1] &quot;numeric&quot; class(4.5) ## [1] &quot;numeric&quot; class(&quot;Fred&quot;) ## [1] &quot;character&quot; class(as.Date(&quot;2021-11-08&quot;)) ## [1] &quot;Date&quot; 2.4.1 Integers By default, R creates double-precision floating-point numeric data objects. To create integer objects: append an L to a constant, e.g. 5L is an integer 5 convert with as.integer We’re going to be looking at various as. functions in R, more on that later, but we should look at as.integer() now. Most other languages use int() for this, and what it does is convert any number into an integer, truncating it to an integer, not rounding it (there’s also a round() function). as.integer(5) ## [1] 5 as.integer(4.5) ## [1] 4 2.5 Rectangular Data A common data format used in most types of research is rectangular data such as in a spreadsheet, with rows and columns, where rows might be observations and columns might be variables (Figure 2.1). We’ll read this type of data in from spreadsheets or even more commonly from comma-separated-variable (CSV) files, though some of these package data sets are already available directly as data frames. FIGURE 2.1: Variables, observations, and values in rectangular data library(igisci) head(sierraFeb) ## # A tibble: 6 × 7 ## STATION_NAME COUNTY ELEVA…¹ LATIT…² LONGI…³ PRECI…⁴ TEMPE…⁵ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GROVELAND 2, CA US Tuolu… 853. 37.8 -120. 176. 6.1 ## 2 CANYON DAM, CA US Plumas 1390. 40.2 -121. 164. 1.4 ## 3 KERN RIVER PH 3, CA US Kern 824. 35.8 -118. 67.1 8.9 ## 4 DONNER MEMORIAL ST PARK, CA US Nevada 1810. 39.3 -120. 167. -0.9 ## 5 BOWMAN DAM, CA US Nevada 1641. 39.5 -121. 277. 2.9 ## 6 BRUSH CREEK RANGER STATION, CA… Butte 1085. 39.7 -121. 296. NA ## # … with abbreviated variable names ¹​ELEVATION, ²​LATITUDE, ³​LONGITUDE, ## # ⁴​PRECIPITATION, ⁵​TEMPERATURE 2.6 Data Structures in R We’ve already started using the most common data structures – scalars and vectors – but haven’t really talked about vectors yet, so we’ll start there. 2.6.1 Vectors A vector is an ordered collection of numbers, strings, vectors, data frames, etc. What we mostly refer to simply as vectors are formally called atomic vectors, which require that they be homogeneous sets of whatever type we’re referring to, such as a vector of numbers, a vector of strings, or a vector of dates/times. You can create a simple vector with the c() function: lats &lt;- c(37.5,47.4,29.4,33.4) lats ## [1] 37.5 47.4 29.4 33.4 states &lt;- c(&quot;VA&quot;, &quot;WA&quot;, &quot;TX&quot;, &quot;AZ&quot;) states ## [1] &quot;VA&quot; &quot;WA&quot; &quot;TX&quot; &quot;AZ&quot; zips &lt;- c(23173, 98801, 78006, 85001) zips ## [1] 23173 98801 78006 85001 The class of a vector is the type of data it holds temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7) class(temp) ## [1] &quot;numeric&quot; Let’s also introduce the handy str() function, which in one step gives you a view of the class of an item and its content – so its structure. We’ll often use it in this book when we want to tell the reader what a data object contains, instead of listing a vector and its class separately, so instead of … temp ## [1] 10.7 9.7 7.7 9.2 7.3 6.7 class(temp) ## [1] &quot;numeric&quot; … we’ll just use str(): str(temp) ## num [1:6] 10.7 9.7 7.7 9.2 7.3 6.7 Vectors can only have one data class, and if mixed with character types, numeric elements will become character: mixed &lt;- c(1, &quot;fred&quot;, 7) str(mixed) ## chr [1:3] &quot;1&quot; &quot;fred&quot; &quot;7&quot; mixed[3] # gets a subset, example of coercion ## [1] &quot;7&quot; 2.6.1.1 NA Data science requires dealing with missing data by storing some sort of null value, called various things: null nodata NA “not available” or “not applicable” as.numeric(c(&quot;1&quot;,&quot;Fred&quot;,&quot;5&quot;)) # note NA introduced by coercion ## [1] 1 NA 5 Note that NA doesn’t really have a data class. The above example created a numeric vector with the one it couldn’t figure out being assigned NA. Remember that vectors (and matrices and arrays) have to be all the same data class. A character vector can also include NAs. Both of the following are valid vectors, with the second item being NA: c(5,NA,7) c(&quot;alpha&quot;,NA,&quot;delta&quot;) ## [1] 5 NA 7 ## [1] &quot;alpha&quot; NA &quot;delta&quot; Note that we typed NA without quotations. It’s kind of like a special constant, like the TRUE and FALSE logical values, neither of which uses quotations. We often want to ignore NA in statistical summaries. Where normally the summary statistic can only return NA… mean(as.numeric(c(&quot;1&quot;, &quot;Fred&quot;, &quot;5&quot;))) ## [1] NA … with na.rm=T you can still get the result for all actual data: mean(as.numeric(c(&quot;1&quot;, &quot;Fred&quot;, &quot;5&quot;)), na.rm=T) ## [1] 3 2.6.1.2 Creating a vector from a sequence We often need sequences of values, and there are a few ways of creating them. The following three examples are equivalent: seq(1,10,1) 1:10 c(1,2,3,4,5,6,7,8,9,10) 2.6.1.3 Vectorization and vector arithmetic Arithmetic on vectors operates element-wise, a process called vectorization. elev &lt;- c(52,394,510,564,725,848,1042,1225,1486,1775,1899,2551) elevft &lt;- elev / 0.3048 elevft ## [1] 170.6037 1292.6509 1673.2283 1850.3937 2378.6089 2782.1522 3418.6352 ## [8] 4019.0289 4875.3281 5823.4908 6230.3150 8369.4226 Another example, with two vectors: temp03 &lt;- c(13.1,11.4,9.4,10.9,8.9,8.4,6.7,7.6,2.8,1.6,1.2,-2.1) temp02 &lt;- c(10.7,9.7,7.7,9.2,7.3,6.7,4.0,5.0,0.9,-1.1,-0.8,-4.4) tempdiff &lt;- temp03 - temp02 tempdiff ## [1] 2.4 1.7 1.7 1.7 1.6 1.7 2.7 2.6 1.9 2.7 2.0 2.3 2.6.1.4 Plotting vectors Vectors of Feb temperature, elevation, and latitude at stations in the Sierra: temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8,-4.4) elev &lt;- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551) lat &lt;- c(39.52,38.91,37.97,38.70,39.09,39.25,39.94,37.75,40.35,39.33,39.17,38.21) Plot individually by index vs a scatterplot We’ll use the plot() function to visualize what’s in a vector. The plot() function will create an output based upon its best guess of what you’re wanting to see, and will depend on the nature of the data you provide it. We’ll be looking at a lot of ways to visualize data soon, but it’s often useful to just see what plot() gives you. In this case, it just makes a bivariate plot where the x dimension is the sequential index of the vector from 1 through the length of the vector, and the values are in the y dimension. For comparison is a scatterplot with elevation on the x axis (Figure 2.2). plot(temp) plot(elev,temp) FIGURE 2.2: Temperature plotted by index (left) and elevation (right) 2.6.2 Lists Lists can be heterogeneous, with multiple class types. Lists are actually used a lot in R, and are created by many operations, but they can be confusing to get used to especially when it’s unclear what we’ll be using them for. We’ll avoid them in this class, but they’re worthwhile learning more about when you have more time. 2.6.3 Matrices Vectors are commonly used as a column in a matrix (or as we’ll see, a data frame), like a variable temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8,-4.4) elev &lt;- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551) lat &lt;- c(39.52,38.91,37.97,38.70,39.09,39.25,39.94,37.75,40.35,39.33,39.17,38.21) Building a matrix from vectors as columns sierradata &lt;- cbind(temp, elev, lat) class(sierradata) ## [1] &quot;matrix&quot; &quot;array&quot; str(sierradata) ## num [1:12, 1:3] 10.7 9.7 7.7 9.2 7.3 6.7 4 5 0.9 -1.1 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:3] &quot;temp&quot; &quot;elev&quot; &quot;lat&quot; sierradata ## temp elev lat ## [1,] 10.7 52 39.52 ## [2,] 9.7 394 38.91 ## [3,] 7.7 510 37.97 ## [4,] 9.2 564 38.70 ## [5,] 7.3 725 39.09 ## [6,] 6.7 848 39.25 ## [7,] 4.0 1042 39.94 ## [8,] 5.0 1225 37.75 ## [9,] 0.9 1486 40.35 ## [10,] -1.1 1775 39.33 ## [11,] -0.8 1899 39.17 ## [12,] -4.4 2551 38.21 2.6.4 Data frames While we can do more things with matrices, R’s data frame object is more useful, and is more similar to the database format that we’re used to in GIS. They’re kind of like a spreadsheet with rules (like the first row is field names) or a matrix that can have variables of unique types. Data frames will be very important for data analysis and GIS. Before we get started, we’re going to use the palmerpenguins data set, so you need to install it if you haven’t yet, and then load the library with: library(palmerpenguins) I’d encourage you to learn more about this dataset at https://allisonhorst.github.io/palmerpenguins/articles/intro.html(Horst, Hill, and Gorman (2020)) (Figures 2.3 and 2.4). It will be useful for a variety of demonstrations using numerical morphometric variables as well as a couple of categorical factors (species and island). FIGURE 2.3: The three penguin species in palmerpenguins. Photos by KB Gorman. Used with permission FIGURE 2.4: Diagram of penguin head with indication of bill length and bill depth (from Horst, Hill, and Gorman (2020), used with permission) Then we can simply display the table with the method: penguins ## # A tibble: 344 × 8 ## species island bill_length_mm bill_depth_mm flipper_…¹ body_…² sex year ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 ## 2 Adelie Torgersen 39.5 17.4 186 3800 fema… 2007 ## 3 Adelie Torgersen 40.3 18 195 3250 fema… 2007 ## 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 ## 5 Adelie Torgersen 36.7 19.3 193 3450 fema… 2007 ## 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 ## 7 Adelie Torgersen 38.9 17.8 181 3625 fema… 2007 ## 8 Adelie Torgersen 39.2 19.6 195 4675 male 2007 ## 9 Adelie Torgersen 34.1 18.1 193 3475 &lt;NA&gt; 2007 ## 10 Adelie Torgersen 42 20.2 190 4250 &lt;NA&gt; 2007 ## # … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm, ## # ²​body_mass_g There are other, fancier table display methods described in https://bookdown.org/igisc/EnvDataSci/ . 2.6.4.1 Creating a data frame out of a matrix There are many functions that start with as. that convert things to a desired type. We’ll use as.data.frame() to create a data frame out of a matrix, the same sierradata we created earlier, but we’ll build it again so it’ll have variable names, and use yet another table display method from the knitr package (which also has a lot of options you might want to explore), which works well for both the html and pdf versions of this book, and creates numbered table headings, so I’ll use it a lot (Table ??). temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8,-4.4) elev &lt;- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551) lat &lt;- c(39.52,38.91,37.97,38.70,39.09,39.25,39.94,37.75,40.35,39.33,39.17,38.21) sierradata &lt;- cbind(temp, elev, lat) mydata &lt;- as.data.frame(sierradata) mydata ## temp elev lat ## 1 10.7 52 39.52 ## 2 9.7 394 38.91 ## 3 7.7 510 37.97 ## 4 9.2 564 38.70 ## 5 7.3 725 39.09 ## 6 6.7 848 39.25 ## 7 4.0 1042 39.94 ## 8 5.0 1225 37.75 ## 9 0.9 1486 40.35 ## 10 -1.1 1775 39.33 ## 11 -0.8 1899 39.17 ## 12 -4.4 2551 38.21 Then to plot the two variables that are now part of the data frame, we’ll need to make vectors out of them again using the $ accessor (Figure 2.5). plot(mydata$elev, mydata$temp) FIGURE 2.5: Temperature and elevation scatter plot 2.6.4.2 Read a data frame from a CSV We’ll be looking at this more in the next chapter, but a common need is to read data from a spreadsheet stored in the CSV format. Normally, you’d have that stored with your project and can just specify the file name, but we’ll access CSVs from the igisci package. Since you have this installed, it will already be on your computer, but not in your project folder. The path to it can be derived using the system.file() function. Reading a csv in readr (part of the tidyverse that we’ll be looking at in the next chapter) is done with read_csv() 2.6 We’ll use the DT::datatable for this, because it lets you interactively scroll across the many variables, but you’ll need to install ‘DT’ to use it; otherwise just try TRI2017 to see what you get. library(readr) csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI/TRI_2017_CA.csv&quot;, package=&quot;igisci&quot;) TRI2017 &lt;- read_csv(csvPath) DT::datatable(TRI2017, options=list(scrollX=T)) FIGURE 2.6: TRI dataframe – DT datatable output Note that we could have used the built-in read.csv function, but as you’ll see later, there are advantages of readr::read_csv so we should get in the habit of using that instead. 2.6.4.3 Reorganizing data frames There are quite a few ways to reorganize your data in R, and we’ll learn other methods in the next chapter where we start using the tidyverse, which makes data abstraction and transformation much easier. For now, we’ll work with a simpler CSV I’ve prepared: csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI/TRI_1987_BaySites.csv&quot;, package=&quot;igisci&quot;) TRI87 &lt;- read_csv(csvPath) TRI87 ## # A tibble: 335 × 9 ## TRI_FACILITY_ID count FACILI…¹ COUNTY air_r…² fugit…³ stack…⁴ LATIT…⁵ LONGI…⁶ ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 91002FRMND585BR 2 FORUM I… SAN M… 1423 1423 0 37.5 -122. ## 2 92052ZPMNF2970C 1 ZEP MFG… SANTA… 337 337 0 37.4 -122. ## 3 93117TLDYN3165P 2 TELEDYN… SANTA… 12600 12600 0 37.4 -122. ## 4 94002GTWSG477HA 2 MORGAN … SAN M… 18700 18700 0 37.5 -122. ## 5 94002SMPRD120SE 2 SEM PRO… SAN M… 1500 500 1000 37.5 -122. ## 6 94025HBLNN151CO 2 HEUBLEI… SAN M… 500 0 500 37.5 -122. ## 7 94025RYCHM300CO 10 TE CONN… SAN M… 144871 47562 97309 37.5 -122. ## 8 94025SNFRD990OB 1 SANFORD… SAN M… 9675 9675 0 37.5 -122. ## 9 94026BYPCK3575H 2 BAY PAC… SAN M… 80000 32000 48000 37.5 -122. ## 10 94026CDRSY3475E 2 CDR SYS… SAN M… 126800 0 126800 37.5 -122. ## # … with 325 more rows, and abbreviated variable names ¹​FACILITY_NAME, ## # ²​air_releases, ³​fugitive_air, ⁴​stack_air, ⁵​LATITUDE, ⁶​LONGITUDE Sort, Index, and Max/Min One simple task is to sort data (numerically or by alphabetic order), such as a variable extracted as a vector. head(sort(TRI87$air_releases)) ## [1] 2 5 5 7 9 10 … or create an index vector of the order of our vector/variable… index &lt;- order(TRI87$air_releases) … where the index vector is just used to store the order of the TRI87$air_releases vector/variable; then we can use that index to display facilities in order of their air releases. head(TRI87$FACILITY_NAME[index]) ## [1] &quot;AIR PRODUCTS MANUFACTURING CORP&quot; ## [2] &quot;UNITED FIBERS&quot; ## [3] &quot;CLOROX MANUFACTURING CO&quot; ## [4] &quot;ICI AMERICAS INC WESTERN RESEARCH CENTER&quot; ## [5] &quot;UNION CARBIDE CORP&quot; ## [6] &quot;SCOTTS-SIERRA HORTICULTURAL PRODS CO INC&quot; This is similar to filtering for a subset. We can also pull out individual values using functions like which.max to find the desired index value: i_max &lt;- which.max(TRI87$air_releases) TRI87$FACILITY_NAME[i_max] # was NUMMI at the time ## [1] &quot;TESLA INC&quot; 2.6.5 Factors Factors are vectors with predefined values, normally used for categorical data, and as R is a statistical language are frequently used to stratify data, such as defining groups for analysis of variance among those groups. They are built on an integer vector, and levels are the set of predefined values, which are commonly character data. nut &lt;- factor(c(&quot;almond&quot;, &quot;walnut&quot;, &quot;pecan&quot;, &quot;almond&quot;)) str(nut) # note that levels will be in alphabetical order ## Factor w/ 3 levels &quot;almond&quot;,&quot;pecan&quot;,..: 1 3 2 1 typeof(nut) ## [1] &quot;integer&quot; As always, there are multiple ways of doing things. Here’s an equivalent conversion that illustrates their relation to integers: nutint &lt;- c(1, 2, 3, 2) # equivalent conversion nut &lt;- factor(nutint, labels = c(&quot;almond&quot;, &quot;pecan&quot;, &quot;walnut&quot;)) str(nut) ## Factor w/ 3 levels &quot;almond&quot;,&quot;pecan&quot;,..: 1 2 3 2 2.6.5.1 Categorical data and factors While character data might be seen as categorical (e.g. \"urban\", \"agricultural\", \"forest\" land covers), to be used as categorical variables they must be made into factors. So we have something to work with, we’ll generate some random memberships in one of three vegetation moisture categories using the sample() function: veg_moisture_categories &lt;- c(&quot;xeric&quot;, &quot;mesic&quot;, &quot;hydric&quot;) veg_moisture_char &lt;- sample(veg_moisture_categories, 42, replace = TRUE) veg_moisture_fact &lt;- factor(veg_moisture_char, levels = veg_moisture_categories) veg_moisture_char ## [1] &quot;xeric&quot; &quot;mesic&quot; &quot;xeric&quot; &quot;xeric&quot; &quot;mesic&quot; &quot;mesic&quot; &quot;hydric&quot; &quot;xeric&quot; ## [9] &quot;xeric&quot; &quot;xeric&quot; &quot;mesic&quot; &quot;hydric&quot; &quot;hydric&quot; &quot;mesic&quot; &quot;xeric&quot; &quot;hydric&quot; ## [17] &quot;mesic&quot; &quot;hydric&quot; &quot;hydric&quot; &quot;xeric&quot; &quot;hydric&quot; &quot;xeric&quot; &quot;xeric&quot; &quot;xeric&quot; ## [25] &quot;hydric&quot; &quot;xeric&quot; &quot;hydric&quot; &quot;mesic&quot; &quot;xeric&quot; &quot;mesic&quot; &quot;xeric&quot; &quot;xeric&quot; ## [33] &quot;xeric&quot; &quot;hydric&quot; &quot;hydric&quot; &quot;hydric&quot; &quot;mesic&quot; &quot;xeric&quot; &quot;mesic&quot; &quot;hydric&quot; ## [41] &quot;mesic&quot; &quot;xeric&quot; veg_moisture_fact ## [1] xeric mesic xeric xeric mesic mesic hydric xeric xeric xeric ## [11] mesic hydric hydric mesic xeric hydric mesic hydric hydric xeric ## [21] hydric xeric xeric xeric hydric xeric hydric mesic xeric mesic ## [31] xeric xeric xeric hydric hydric hydric mesic xeric mesic hydric ## [41] mesic xeric ## Levels: xeric mesic hydric To make a categorical variable a factor: nut &lt;- c(&quot;almond&quot;, &quot;walnut&quot;, &quot;pecan&quot;, &quot;almond&quot;) farm &lt;- c(&quot;organic&quot;, &quot;conventional&quot;, &quot;organic&quot;, &quot;organic&quot;) ag &lt;- as.data.frame(cbind(nut, farm)) ag$nut &lt;- factor(ag$nut) ag$nut ## [1] almond walnut pecan almond ## Levels: almond pecan walnut Factor example library(igisci) sierraFeb$COUNTY &lt;- factor(sierraFeb$COUNTY) str(sierraFeb$COUNTY) ## Factor w/ 21 levels &quot;Amador&quot;,&quot;Butte&quot;,..: 20 14 7 12 12 2 19 11 2 19 ... 2.7 Accessors and Subsetting The use of accessors in R can be confusing, but they’re very important to understand, especially for base R. An accessor is “a method for accessing data in an object usually an attribute of that object” (Brown (n.d.)), so a method for subsetting, and for R these are [], [[]], and $, but it can be confusing to know when you might use which one. There are good reasons to have these three types for code clarity, however you can also use [] with a bit of clumsiness for all purposes. To learn more about accessors and subsetting using base R, see the longer treatment in https://bookdown.org/igisc/EnvDataSci/ . 2.7.1 $ Accessing a vector from a data frame The $ accessor is really just a shortcut, but any shortcut reduces code and thus increases clarity, so it’s a good idea and this accessor is commonly used. Their only limitation is that you can’t use the integer indices, which would allow you to loop through a numerical sequence. These accessor operations do the same thing: cars$speed cars[,&quot;speed&quot;] cars[[&quot;speed&quot;]] ## num [1:50] 4 4 7 7 8 9 10 10 10 11 ... 2.8 Programming scripts in RStudio Given the exploratory nature of the R language, we sometimes forget that it provides significant capabilities as a programming language where we can solve more complex problems by coding procedures and using logic to control the process and handle a range of possible scenarios. One programming structure is defining your own functions. One simple function I found useful for our external data in igisci is to simplify the code needed to access the external data. I found I had to keep looking up the syntax for that task that we use a lot. It also makes the code difficult to read. Adding this function to the top of your code helps for both: ex &lt;- function(fnam){system.file(&quot;extdata&quot;,fnam,package=&quot;igisci&quot;)} Then our code that accesses data is greatly simplified, with read.csv calls looking a lot like reading data stored in our project folder. For example, where if we had fishdata.csv stored locally in our project folder we might read it with … read.csv(\"fishdata.csv\") … reading from the data package’s extdata folder looks pretty similar: read.csv(ex(\"fishdata.csv\")) This simple function was so useful that it’s now included in the igisci package, so you can just call it with ex() if you have library(igisci) in your code. To learn more about R programming structures, see the longer treatment in https://bookdown.org/igisc/EnvDataSci/ . 2.8.1 Apply functions There are many apply functions in R, and they often obviate the need for looping (see the for loop in the longer book). For instance: apply derives values at margins of rows and columns, e.g. to sum across rows or down columns. # matrix apply – the same would apply to data frames matrix12 &lt;- 1:12 dim(matrix12) &lt;- c(3,4) rowsums &lt;- apply(matrix12, 1, sum) colsums &lt;- apply(matrix12, 2, sum) sum(rowsums) ## [1] 78 sum(colsums) ## [1] 78 zero &lt;- sum(rowsums) - sum(colsums) matrix12 ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Apply functions satisfy one of the needs that spreadsheets are used for. Consider how often you use sum, mean, or similar functions in Excel. sapply sapply applies functions to either: all elements of a vector – unary functions only sapply(1:12, sqrt) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 3.316625 3.464102 or all variables of a data frame (not a matrix), where it works much like a column-based apply (since variables are columns) but more easily interpreted without the need of specifying columns with 2: sapply(cars,mean) # same as apply(cars,2,mean) ## speed dist ## 15.40 42.98 temp02 &lt;- c(10.7,9.7,7.7,9.2,7.3,6.7,4.0,5.0,0.9,-1.1,-0.8,-4.4) temp03 &lt;- c(13.1,11.4,9.4,10.9,8.9,8.4,6.7,7.6,2.8,1.6,1.2,-2.1) sapply(as.data.frame(cbind(temp02,temp03)),mean) # has to be a data frame ## temp02 temp03 ## 4.575000 6.658333 While various apply functions are in base R, the purrr package takes these further. See https://www.rstudio.com/resources/cheatsheets/ for more information on this and other packages in the RStudio/tidyverse world. 2.9 RStudio projects So far, you’ve been using RStudio, and it organizes your code and data into a project folder. You should familiarize yourself with where things are being saved and where you can find things. Start by seeing your working directory with : getwd() When you create a new RStudio project with File/New Project..., it will set the working directory to the project folder, where you create the project. (You can change the working directory with setwd() but I don’t recommend it.) The project folder is useful for keeping things organized and allowing you to use relative paths to your data and allow everything to be moved somewhere else and still work. The project file has the extension .Rproj and it will reside in the project folder. If you’ve saved any scripts (.R)or R Markdown (.Rmd) documents, they’ll also reside in the project folder; and if you’ve saved any data, or if you want to read any data without providing the full path or using the extdata access method, those data files (e.g. .csv) will be in that project folder. You can see your scripts, R Markdown documents, and data files using the Files tab in the default lower right pane of RStudio. RStudio projects are going to be the way we’ll want to work for the rest of this book, so you’ll often want to create new ones for particular data sets so things don’t get messy. And you may want to create data folders within your project folder, as we have in the igisci extdata folder, to keep things organized. Since we’re using our igisci package, this is less of an issue since at least input data aren’t stored in the project folder. However you’re going to be creating data, so you’ll want to manage your data in individual projects. You may want to start a new project for each data set, using File/New Project, and try to keep things organized (things can get messy fast!) In this book, we’ll be making a lot of use of data provided for you from various data packages such as built-in data, palmerpenguins (Horst, Hill, and Gorman 2020), or igisci, but they correspond to specific research projects, such as Sierra Climate to which several data frames and spatial data apply. For this chapter, you can probably just use one project, but later you’ll find it useful to create separate projects for each data set – such as a sierra project and return to it every time it applies. In that project, you’ll build a series of scripts, many of which you’ll re-use to develop new methods. When you’re working on your own project with your own data files, you should store these in a data folder inside the project folder. With the project folder as the default working directory, you can use relative paths, and everything will work even if the project folder is moved. So, for instance, you can specify \"data/mydata.csv\" as the path to a csv of that name. You can still access package data, including extdata folders and files, but your processed and saved or imported data will reside with your project. An absolute path to somewhere on your computer in contrast won’t work for anyone else trying to run your code; absolute paths should only be used for servers that other users have access to and URLs on the web. 2.9.1 R Markdown An alternative to writing scripts is writing R Markdown documents, which includes both formatted text (such as you’re seeing in this book, like italics created using asterisks) and code chunks. R lends itself to running code in chunks, as opposed to creating complete tools that run all of the way through. This book is built from R Markdown documents organized in a bookdown structure, and most of the figures are created from R code chunks. There are also many good resources on writing R Markdown documents, including the very thorough R Markdown: The Definitive Guide (Xie, Allaire, and Grolemund 2019). 2.10 Exercises: Introduction to R Exercise 2.1 Assign scalars for your name, city, state and zip code, and use paste() to combine them, and assign them to the object me. What is the class of me? Exercise 2.2 You can create a vector uniform random numbers from 0 to 1 using runif(n=30) where n=30 says to make 30 of them. Use the round() function to round each of the values (it vectorizes them), and provide what you created and explain what happened. Exercise 2.3 Create two vectors x and y of 10 numbers each with the c() function, then assigning to x and y. Then plot(x,y), and provide the three lines of code you used to do the assignment and plot. Exercise 2.4 Referring to the Matrices section, create the same sierradata matrix using the same data vectors repeated here … … then convert it to a data frame (using the same sierradata object name), and from that data frame plot temperature (temp) against latitude (lat). Exercise 2.5 From that sierradata data frame, derive colmeans using the mean parameter on the columns 2 for apply(). Exercise 2.6 Do the same thing with the sierra data frame with sapply(). References "],["abstraction.html", "Chapter 3 Data Abstraction 3.1 The Tidyverse 3.2 Tibbles 3.3 Summarizing variable distributions 3.4 Database operations with dplyr 3.5 String abstraction 3.6 Calling functions explicitly with :: 3.7 Exercises: Data Abstraction", " Chapter 3 Data Abstraction Abstracting data from large data sets (or even small ones) is critical to data science. The most common first step to visualization is abstracting the data in a form that allows for the visualization goal in mind. If you’ve ever worked with data in spreadsheets, you commonly will be faced with some kind of data manipulation to create meaningful graphs, unless that spreadsheet is specifically designed for it, but then doing something else with the data is going to require some work. FIGURE 3.1: Visualization of some abstracted data from the EPA Toxic Release Inventory Figure 3.1 started with abstracting some data from EPA’s Toxic Release Inventory (TRI) program, which holds data reported from a large number of facilities that must report either “stack” or “fugitive” air. Some of the abstraction had already happened when I used the EPA website to download data for particular years and only in California. But there’s more we need to do, and we’ll want to use some dplyr functions to help with it. At this point, we’ve learned the basics of working with the R language. From here we’ll want to explore how to analyze data, statistically, spatially, and temporally. One part of this is abstracting information from existing data sets by selecting variables and observations and summarizing their statistics. In the previous chapter, we learned some abstraction methods in base R, such as selecting parts of data frames and applying some functions across the data frame. There’s a lot we can do with these methods, and we’ll continue to use them, but they can employ some fairly arcane language. There are many packages that extend R’s functionality, but some of the most important for data science can be found in the various packages of “The Tidyverse” (Wickham and Grolemund 2016), which has the philosophy of making data manipulation more intuitive. We’ll start with dplyr, which includes an array of data manipulation tools, including select for selecting variables, filter for subsetting observations, summarize for reducing variables to summary statistics, typically stratified by groups, and mutate for creating new variables from mathematical expressions from existing variables. Some dplyr tools such as data joins we’ll look at later in the data transformation chapter. 3.1 The Tidyverse The tidyverse refers to a suite of R packages developed at RStudio (see https://rstudio.com and &lt;https://r4ds.had.co.nz&gt;) for facilitating data processing and analysis. While R itself is designed around exploratory data analysis, the tidyverse takes it further. Some of the packages in the tidyverse that are widely used are: dplyr : data manipulation like a database readr : better methods for reading and writing rectangular data tidyr : reorganization methods that extend dplyr’s database capabilities purrr : expanded programming toolkit including enhanced “apply” methods tibble : improved data frame stringr : string manipulation library ggplot2 : graphing system based on the grammar of graphics In this chapter, we’ll be mostly exploring dplyr, with a few other things thrown in like reading data frames with readr. For simplicity, we can just include library(tidyverse) to get everything. 3.2 Tibbles Tibbles are an improved type of data frame part of the tidyverse serve the same purpose as a data frame, and all data frame operations work Advantages display better can be composed of more complex objects like lists, etc. can be grouped There multiple ways to create a tibble: Reading from a CSV using read_csv(). Note the underscore, a function naming convention in the tidyverse. Using tibble() to either build from vectors or from scratch, or convert from a different type of data frame. Using tribble() to build in code from scratch. Using various tidyverse functions that return tibbles. 3.2.1 Building a tibble from vectors We’ll start by looking at a couple of built-in character vectors (there are lots of things like this in R): letters : lowercase letters LETTERS : uppercase letters letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; LETTERS ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; &quot;K&quot; &quot;L&quot; &quot;M&quot; &quot;N&quot; &quot;O&quot; &quot;P&quot; &quot;Q&quot; &quot;R&quot; &quot;S&quot; ## [20] &quot;T&quot; &quot;U&quot; &quot;V&quot; &quot;W&quot; &quot;X&quot; &quot;Y&quot; &quot;Z&quot; … then make a tibble of letters, LETTERS, and two random sets of 26 values, one normally distributed, the other uniform: norm &lt;- rnorm(26) unif &lt;- runif(26) library(tidyverse) tibble26 &lt;- tibble(letters,LETTERS,norm,unif) tibble26 ## # A tibble: 26 × 4 ## letters LETTERS norm unif ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a A -1.21 0.0894 ## 2 b B 0.682 0.0967 ## 3 c C 0.907 0.763 ## 4 d D 0.807 0.665 ## 5 e E -1.63 0.925 ## 6 f F -1.06 0.747 ## 7 g G -0.0797 0.227 ## 8 h H -0.359 0.956 ## 9 i I -1.21 0.811 ## 10 j J -0.493 0.832 ## # … with 16 more rows See section ?? for more on creating random (or rather pseudo-random) numbers in R. 3.2.2 tribble As long as you don’t let them multiply in your starship, tribbles are handy for creating tibbles. (Or rather the tribble function is a handy way to create tibbles in code.) You simply create the variable names with a series of entries starting with a tilde, then the data are entered one row at a time. If you line them all up in your code one row at a time, it’s easy to enter the data accurately (Table 3.1). peaks &lt;- tribble( ~peak, ~elev, ~longitude, ~latitude, &quot;Mt. Whitney&quot;, 4421, -118.2, 36.5, &quot;Mt. Elbert&quot;, 4401, -106.4, 39.1, &quot;Mt. Hood&quot;, 3428, -121.7, 45.4, &quot;Mt. Rainier&quot;, 4392, -121.8, 46.9) knitr::kable(peaks, caption = &#39;Peaks tibble&#39;) TABLE 3.1: Peaks tibble peak elev longitude latitude Mt. Whitney 4421 -118.2 36.5 Mt. Elbert 4401 -106.4 39.1 Mt. Hood 3428 -121.7 45.4 Mt. Rainier 4392 -121.8 46.9 3.2.3 read_csv The read_csv function does somewhat the same thing as read.csv in base R, but creates a tibble instead of a data.frame, and has some other properties we’ll look at below. Note that the code below accesses data we’ll be using a lot, from EPA Toxic Release Inventory (TRI) data. If you want to keep this data organized in a separate project, you might consider creating a new air_quality project. This is optional, and you can get by with staying in one project since all of our data will be accessed from the igisci package. But in your own work, you will find it useful to create separate projects to keep things organized with your code and data together. library(tidyverse); library(igisci) TRI87 &lt;- read_csv(ex(&quot;TRI/TRI_1987_BaySites.csv&quot;)) TRI87df &lt;- read.csv(ex(&quot;TRI/TRI_1987_BaySites.csv&quot;)) TRI87b &lt;- tibble(TRI87df) identical(TRI87, TRI87b) ## [1] FALSE Note that they’re not identical. So what’s the difference between read_csv and read.csv? Why would we use one over the other? Since their names are so similar, you may accidentally choose one or the other. Some things to consider: To use read_csv, you need to load the readr or tidyverse library, or use readr::read_csv. The read.csv function “fixes” some things and sometimes that might be desired: problematic variable names like MLY-TAVG-NORMAL become MLY.TAVG.NORMAL – but this may create problems if those original names are a standard designation. With read.csv, numbers stored as characters are converted to numbers: “01” becomes 1, “02” becomes 2, etc. There are other known problems that read_csv avoids. Recommendation: Use read_csv and write_csv. You can still just call tibbles “data frames”, since they are still data frames, and in this book we’ll follow that practice. 3.3 Summarizing variable distributions A simple statistical summary is very easy to do in R, and we’ll use eucoak data in the igisci package from a study of comparative runoff and erosion under eucalyptus and oak canopies (Thompson, Davis, and Oliphant 2016). In this study, we looked at the amount of runoff and erosion captured in Gerlach troughs on paired eucalyptus and oak sites in the San Francisco Bay Area. Euc-Oak paired plot runoff and erosion study (Thompson, Davis, and Oliphant (2016)) library(igisci) summary(eucoakrainfallrunoffTDR) ## site site # date month ## Length:90 Min. :1.000 Length:90 Length:90 ## Class :character 1st Qu.:2.000 Class :character Class :character ## Mode :character Median :4.000 Mode :character Mode :character ## Mean :4.422 ## 3rd Qu.:6.000 ## Max. :8.000 ## ## rain_mm rain_oak rain_euc runoffL_oak ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 0.000 ## 1st Qu.:16.00 1st Qu.:16.00 1st Qu.:14.75 1st Qu.: 0.000 ## Median :28.50 Median :30.50 Median :30.00 Median : 0.450 ## Mean :37.99 Mean :35.08 Mean :34.60 Mean : 2.032 ## 3rd Qu.:63.25 3rd Qu.:50.50 3rd Qu.:50.00 3rd Qu.: 2.800 ## Max. :99.00 Max. :98.00 Max. :96.00 Max. :14.000 ## NA&#39;s :18 NA&#39;s :2 NA&#39;s :2 NA&#39;s :5 ## runoffL_euc slope_oak slope_euc aspect_oak ## Min. : 0.00 Min. : 9.00 Min. : 9.00 Min. :100.0 ## 1st Qu.: 0.07 1st Qu.:12.00 1st Qu.:12.00 1st Qu.:143.0 ## Median : 1.20 Median :24.50 Median :23.00 Median :189.0 ## Mean : 2.45 Mean :21.62 Mean :19.34 Mean :181.9 ## 3rd Qu.: 3.30 3rd Qu.:30.50 3rd Qu.:25.00 3rd Qu.:220.0 ## Max. :16.00 Max. :32.00 Max. :31.00 Max. :264.0 ## NA&#39;s :3 ## aspect_euc surface_tension_oak surface_tension_euc ## Min. :106.0 Min. :37.40 Min. :28.51 ## 1st Qu.:175.0 1st Qu.:72.75 1st Qu.:32.79 ## Median :196.5 Median :72.75 Median :37.40 ## Mean :191.2 Mean :68.35 Mean :43.11 ## 3rd Qu.:224.0 3rd Qu.:72.75 3rd Qu.:56.41 ## Max. :296.0 Max. :72.75 Max. :72.75 ## NA&#39;s :22 NA&#39;s :22 ## runoff_rainfall_ratio_oak runoff_rainfall_ratio_euc ## Min. :0.00000 Min. :0.000000 ## 1st Qu.:0.00000 1st Qu.:0.003027 ## Median :0.02046 Median :0.047619 ## Mean :0.05357 Mean :0.065902 ## 3rd Qu.:0.08485 3rd Qu.:0.083603 ## Max. :0.42000 Max. :0.335652 ## NA&#39;s :5 NA&#39;s :3 In the summary output, how are character variables handled differently from numeric ones? Remembering what we discussed in the previous chapter, consider the site variable (Figure 3.2), and in particular its Length. Looking at the table, what does that length represent? FIGURE 3.2: Eucalyptus/Oak paired site locations There are a couple of ways of seeing what unique values exist in a character variable like site which can be considered a categorical variable (factor). Consider what these return: unique(eucoakrainfallrunoffTDR$site) ## [1] &quot;AB1&quot; &quot;AB2&quot; &quot;KM1&quot; &quot;PR1&quot; &quot;TP1&quot; &quot;TP2&quot; &quot;TP3&quot; &quot;TP4&quot; factor(eucoakrainfallrunoffTDR$site) ## [1] AB1 AB1 AB1 AB1 AB1 AB1 AB1 AB1 AB1 AB1 AB1 AB1 AB2 AB2 AB2 AB2 AB2 AB2 AB2 ## [20] AB2 AB2 AB2 AB2 AB2 KM1 KM1 KM1 KM1 KM1 KM1 KM1 KM1 KM1 KM1 KM1 KM1 PR1 PR1 ## [39] PR1 PR1 PR1 PR1 PR1 PR1 PR1 PR1 TP1 TP1 TP1 TP1 TP1 TP1 TP1 TP1 TP1 TP1 TP1 ## [58] TP2 TP2 TP2 TP2 TP2 TP2 TP2 TP2 TP2 TP2 TP2 TP3 TP3 TP3 TP3 TP3 TP3 TP3 TP3 ## [77] TP3 TP3 TP3 TP4 TP4 TP4 TP4 TP4 TP4 TP4 TP4 TP4 TP4 TP4 ## Levels: AB1 AB2 KM1 PR1 TP1 TP2 TP3 TP4 3.3.1 Stratifying variables by site using a Tukey box plot A good way to look at variable distributions stratified by a sample site factor is the Tukey box plot (Figure 3.3). We’ll be looking more at this and other visualization methods in the next chapter. ggplot(data = eucoakrainfallrunoffTDR) + geom_boxplot(mapping = aes(x=site, y=runoffL_euc)) FIGURE 3.3: Tukey boxplot of runoff under eucalyptus canopy 3.4 Database operations with dplyr As part of exploring our data, we’ll typically simplify or reduce it for our purposes. The following methods are quickly discovered to be essential as part of exploring and analyzing data. select rows using logic, such as population \\&gt; 10000, with filter select variable columns you want to retain with select add new variables and assign their values with mutate sort rows based on a field with arrange summarize by group 3.4.1 Select, mutate, and the pipe Read the pipe operator %&gt;% as “and then…” This is bigger than it sounds and opens up many possibilities. See the example below, and observe how the expression becomes several lines long. In the process, we’ll see examples of new variables with mutate and selecting (and in the process ordering) variables (Table 3.2). runoff &lt;- eucoakrainfallrunoffTDR %&gt;% mutate(Date = as.Date(date,&quot;%m/%d/%Y&quot;), rain_subcanopy = (rain_oak + rain_euc)/2) %&gt;% dplyr::select(site, Date, rain_mm, rain_subcanopy, runoffL_oak, runoffL_euc, slope_oak, slope_euc) TABLE 3.2: EucOak data reorganized a bit, first 6 site Date rain_mm rain_subcanopy runoffL_oak runoffL_euc slope_oak slope_euc AB1 2006-11-08 29 29.0 4.7900 6.70000 32 31 AB1 2006-11-12 22 18.5 3.2000 4.30000 32 31 AB1 2006-11-29 85 65.0 9.7000 16.00000 32 31 AB1 2006-12-12 82 87.5 14.0000 14.20000 32 31 AB1 2006-12-28 43 54.0 9.7472 4.32532 32 31 AB1 2007-01-29 7 54.0 1.4000 0.00000 32 31 Another way of thinking of the pipe that is very useful is that whatever goes before it becomes the first parameter for any functions that follow. So in the example above: The parameter eucoakrainfallrunoffTDR becomes the first for mutate(), then The result of the mutate() becomes the first parameter for dplyr::select() To just rename a variable, use rename instead of mutate. It will stay in position. 3.4.1.1 Review: creating penguins from penguins_raw To review some of these methods, it’s useful to consider how the penguins data frame was created from the more complex penguins_raw data frame, both of which are part of the palmerpenguins package (Horst, Hill, and Gorman (2020)). First let’s look at palmerpenguins::penguins_raw: library(palmerpenguins) library(tidyverse) library(lubridate) summary(penguins_raw) ## studyName Sample Number Species Region ## Length:344 Min. : 1.00 Length:344 Length:344 ## Class :character 1st Qu.: 29.00 Class :character Class :character ## Mode :character Median : 58.00 Mode :character Mode :character ## Mean : 63.15 ## 3rd Qu.: 95.25 ## Max. :152.00 ## ## Island Stage Individual ID Clutch Completion ## Length:344 Length:344 Length:344 Length:344 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## Date Egg Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) ## Min. :2007-11-09 Min. :32.10 Min. :13.10 Min. :172.0 ## 1st Qu.:2007-11-28 1st Qu.:39.23 1st Qu.:15.60 1st Qu.:190.0 ## Median :2008-11-09 Median :44.45 Median :17.30 Median :197.0 ## Mean :2008-11-27 Mean :43.92 Mean :17.15 Mean :200.9 ## 3rd Qu.:2009-11-16 3rd Qu.:48.50 3rd Qu.:18.70 3rd Qu.:213.0 ## Max. :2009-12-01 Max. :59.60 Max. :21.50 Max. :231.0 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :2 ## Body Mass (g) Sex Delta 15 N (o/oo) Delta 13 C (o/oo) ## Min. :2700 Length:344 Min. : 7.632 Min. :-27.02 ## 1st Qu.:3550 Class :character 1st Qu.: 8.300 1st Qu.:-26.32 ## Median :4050 Mode :character Median : 8.652 Median :-25.83 ## Mean :4202 Mean : 8.733 Mean :-25.69 ## 3rd Qu.:4750 3rd Qu.: 9.172 3rd Qu.:-25.06 ## Max. :6300 Max. :10.025 Max. :-23.79 ## NA&#39;s :2 NA&#39;s :14 NA&#39;s :13 ## Comments ## Length:344 ## Class :character ## Mode :character ## ## ## ## Now let’s create the simpler penguins data frame. We’ll use rename for a couple, but most variables require mutation to manipulate strings (we’ll get to that later), create factors, or convert to integers. And we’ll rename some variables to avoid using backticks (the backward single quotation mark accessed just to the left of the 1 key and below the Esc key, and what you can use in markdown to create a monospaced font as I just used for 1 and Esc). penguins &lt;- penguins_raw %&gt;% rename(bill_length_mm = `Culmen Length (mm)`, bill_depth_mm = `Culmen Depth (mm)`) %&gt;% mutate(species = factor(word(Species)), island = factor(Island), flipper_length_mm = as.integer(`Flipper Length (mm)`), body_mass_g = as.integer(`Body Mass (g)`), sex = factor(str_to_lower(Sex)), year = as.integer(year(ymd(`Date Egg`)))) %&gt;% dplyr::select(species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, year) summary(penguins) ## species island bill_length_mm bill_depth_mm ## Adelie :152 Biscoe :168 Min. :32.10 Min. :13.10 ## Chinstrap: 68 Dream :124 1st Qu.:39.23 1st Qu.:15.60 ## Gentoo :124 Torgersen: 52 Median :44.45 Median :17.30 ## Mean :43.92 Mean :17.15 ## 3rd Qu.:48.50 3rd Qu.:18.70 ## Max. :59.60 Max. :21.50 ## NA&#39;s :2 NA&#39;s :2 ## flipper_length_mm body_mass_g sex year ## Min. :172.0 Min. :2700 female:165 Min. :2007 ## 1st Qu.:190.0 1st Qu.:3550 male :168 1st Qu.:2007 ## Median :197.0 Median :4050 NA&#39;s : 11 Median :2008 ## Mean :200.9 Mean :4202 Mean :2008 ## 3rd Qu.:213.0 3rd Qu.:4750 3rd Qu.:2009 ## Max. :231.0 Max. :6300 Max. :2009 ## NA&#39;s :2 NA&#39;s :2 Unfortunately, they don’t end up as exactly identical, though all of the variables are identical as vectors: identical(penguins, palmerpenguins::penguins) ## [1] FALSE 3.4.1.2 Helper functions for dplyr::select() In the select() example above, we listed all of the variables, but there are a variety of helper functions for using logic to specify which variables to select. Here are a few: contains(\"_\") or any substring of interest in the variable name starts_with(\"runoff\") ends_with(\"euc\") num_range(\"x\",1:5) for the common situation where a series of variable names combine a string and a number range of variables: e.g. runoffL_oak:slope_euc could have followed rain_subcanopy above all but: preface a variable or a set of variable names with - to select all others 3.4.2 filter filter lets you select observations that meet criteria, similar to an SQL WHERE clause (Table 3.3). runoff2007 &lt;- runoff %&gt;% filter(Date &gt;= as.Date(&quot;04/01/2007&quot;, &quot;%m/%d/%Y&quot;)) TABLE 3.3: Date-filtered EucOak data site Date rain_mm rain_subcanopy runoffL_oak runoffL_euc slope_oak slope_euc AB1 2007-04-23 NA 33.5 6.94488 9.19892 32.0 31 AB1 2007-05-05 NA 31.0 6.33568 7.43224 32.0 31 AB2 2007-04-23 23 35.5 4.32000 2.88000 24.0 25 AB2 2007-05-05 11 25.5 4.98000 3.30000 24.0 25 KM1 2007-04-23 NA 37.0 1.56000 2.04000 30.5 25 KM1 2007-05-05 28 22.0 1.32000 1.32000 30.5 25 3.4.2.1 Filtering out NA with !is.na Here’s a really important one. There are many times you need to avoid NAs. We thus commonly see summary statistics using na.rm = TRUE in order to ignore NAs when calculating a statistic like mean. To simply filter out NAs from a vector or a variable use a filter: feb_filt &lt;- sierraFeb %&gt;% filter(!is.na(TEMPERATURE)) 3.4.3 Writing a data frame to a csv Let’s say you have created a data frame, maybe with read_csv runoff20062007 &lt;- read_csv(ex(\"eucoak/eucoakrainfallrunoffTDR.csv\")) Then you do some processing to change it, maybe adding variables, reorganizing, etc., and you want to write out your new eucoak, so you just need to use write_csv write_csv(eucoak, \"data/tidy_eucoak.csv\") Note the use of a data folder data: Remember that your default workspace (wd for working directory) is where your project file resides (check what it is with getwd()), so by default you’re saving things in that wd. To keep things organized the above code is placing data in a data folder within the wd. 3.4.4 Summarize by group You’ll find that you need to use this all the time with real data. Let’s say you have a bunch of data where some categorical variable is defining a grouping, like our site field in the eucoak data. This is a form of stratifying our data. We’d like to just create average slope, rainfall, and runoff for each site. Note that it involves two steps, first defining which field defines the group, then the various summary statistics we’d like to store. In this case, all of the slopes under oak remain the same for a given site – it’s a site characteristic – and the same applies to the euc site, so we can just grab the first value (mean would have also worked of course) (Table 3.4). eucoakSiteAvg &lt;- runoff %&gt;% group_by(site) %&gt;% summarize( rain = mean(rain_mm, na.rm = TRUE), rain_subcanopy = mean(rain_subcanopy, na.rm = TRUE), runoffL_oak = mean(runoffL_oak, na.rm = TRUE), runoffL_euc = mean(runoffL_euc, na.rm = TRUE), slope_oak = first(slope_oak), slope_euc = first(slope_euc) ) TABLE 3.4: EucOak data summarized by site site rain rain_subcanopy runoffL_oak runoffL_euc slope_oak slope_euc AB1 48.37500 43.08333 6.8018364 6.026523 32.0 31 AB2 34.08333 35.37500 4.9113636 3.654545 24.0 25 KM1 48.00000 36.12500 1.9362500 0.592500 30.5 25 PR1 56.50000 37.56250 0.4585714 2.310000 27.0 23 TP1 38.36364 30.04545 0.8772727 1.657273 9.0 9 TP2 34.33333 32.86364 0.0954545 1.525454 12.0 10 Summarizing by group with TRI data library(igisci) TRI_BySite &lt;- read_csv(ex(&quot;TRI/TRI_2017_CA.csv&quot;)) %&gt;% mutate(all_air = `5.1_FUGITIVE_AIR` + `5.2_STACK_AIR`) %&gt;% filter(all_air &gt; 0) %&gt;% group_by(FACILITY_NAME) %&gt;% summarize( FACILITY_NAME = first(FACILITY_NAME), air_releases = sum(all_air, na.rm = TRUE), mean_fugitive = mean(`5.1_FUGITIVE_AIR`, na.rm = TRUE), LATITUDE = first(LATITUDE), LONGITUDE = first(LONGITUDE)) 3.4.5 Count The count function is a simple variant on summarizing by group, since the only statistic is the count of events. See https://bookdown.org/igisc/EnvDataSci/ for more on this. tidy_eucoak %&gt;% count(tree) ## # A tibble: 2 × 2 ## tree n ## &lt;chr&gt; &lt;int&gt; ## 1 euc 90 ## 2 oak 90 3.4.6 Sorting after summarizing Using the marine debris data from the Marine Debris Monitoring and Assessment Project (Marine Debris Program, n.d.), we can use arrange to sort by latitude, so we can see the beaches from south to north along the Pacific coast. shorelineLatLong &lt;- ConcentrationReport %&gt;% group_by(`Shoreline Name`) %&gt;% summarize( latitude = mean((`Latitude Start`+`Latitude End`)/2), longitude = mean((`Longitude Start`+`Longitude End`)/2) ) %&gt;% arrange(latitude) shorelineLatLong ## # A tibble: 38 × 3 ## `Shoreline Name` latitude longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aimee Arvidson 33.6 -118. ## 2 Balboa Pier #2 33.6 -118. ## 3 Bolsa Chica 33.7 -118. ## 4 Junipero Beach 33.8 -118. ## 5 Malaga Cove 33.8 -118. ## 6 Zuma Beach, Malibu 34.0 -119. ## 7 Zuma Beach 34.0 -119. ## 8 Will Rodgers 34.0 -119. ## 9 Carbon Beach 34.0 -119. ## 10 Nicholas Canyon 34.0 -119. ## # … with 28 more rows 3.5 String abstraction Character string manipulation is surprisingly critical to data analysis, and so the stringr package was developed to provide a wider array of string processing tools than what is in base R, including functions for detecting matches, subsetting strings, managing lengths, replacing substrings with other text, and joining, splitting, and sorting strings. We’ll look at just a couple of stringr functions (there is more coverage in https://bookdown.org/igisc/EnvDataSci ). We’ll use a built-in dataset of fruit names: fruit. library(stringr) fruit ## [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; ## [4] &quot;banana&quot; &quot;bell pepper&quot; &quot;bilberry&quot; ## [7] &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; ## [10] &quot;blueberry&quot; &quot;boysenberry&quot; &quot;breadfruit&quot; ## [13] &quot;canary melon&quot; &quot;cantaloupe&quot; &quot;cherimoya&quot; ## [16] &quot;cherry&quot; &quot;chili pepper&quot; &quot;clementine&quot; ## [19] &quot;cloudberry&quot; &quot;coconut&quot; &quot;cranberry&quot; ## [22] &quot;cucumber&quot; &quot;currant&quot; &quot;damson&quot; ## [25] &quot;date&quot; &quot;dragonfruit&quot; &quot;durian&quot; ## [28] &quot;eggplant&quot; &quot;elderberry&quot; &quot;feijoa&quot; ## [31] &quot;fig&quot; &quot;goji berry&quot; &quot;gooseberry&quot; ## [34] &quot;grape&quot; &quot;grapefruit&quot; &quot;guava&quot; ## [37] &quot;honeydew&quot; &quot;huckleberry&quot; &quot;jackfruit&quot; ## [40] &quot;jambul&quot; &quot;jujube&quot; &quot;kiwi fruit&quot; ## [43] &quot;kumquat&quot; &quot;lemon&quot; &quot;lime&quot; ## [46] &quot;loquat&quot; &quot;lychee&quot; &quot;mandarine&quot; ## [49] &quot;mango&quot; &quot;mulberry&quot; &quot;nectarine&quot; ## [52] &quot;nut&quot; &quot;olive&quot; &quot;orange&quot; ## [55] &quot;pamelo&quot; &quot;papaya&quot; &quot;passionfruit&quot; ## [58] &quot;peach&quot; &quot;pear&quot; &quot;persimmon&quot; ## [61] &quot;physalis&quot; &quot;pineapple&quot; &quot;plum&quot; ## [64] &quot;pomegranate&quot; &quot;pomelo&quot; &quot;purple mangosteen&quot; ## [67] &quot;quince&quot; &quot;raisin&quot; &quot;rambutan&quot; ## [70] &quot;raspberry&quot; &quot;redcurrant&quot; &quot;rock melon&quot; ## [73] &quot;salal berry&quot; &quot;satsuma&quot; &quot;star fruit&quot; ## [76] &quot;strawberry&quot; &quot;tamarillo&quot; &quot;tangerine&quot; ## [79] &quot;ugli fruit&quot; &quot;watermelon&quot; Then just a couple of simple but very useful stringr functions: (1) subsetting a list of strings to those with a “q” in them qfruit &lt;- str_subset(fruit,&quot;q&quot;) qfruit ## [1] &quot;kumquat&quot; &quot;loquat&quot; &quot;quince&quot; and (2) replacing all “q”s with “z”s: str_replace(qfruit,&quot;q&quot;,&quot;z&quot;) ## [1] &quot;kumzuat&quot; &quot;lozuat&quot; &quot;zuince&quot; There are also base R methods that work well, for instance paste and paste0 that concatenate strings (similar to stringr’s str_c), either with a space padding (paste) or not (paste0). phrase &lt;- paste(&quot;for&quot;,&quot;whom&quot;,&quot;the&quot;,&quot;bell&quot;,&quot;tolls&quot;) phrase ## [1] &quot;for whom the bell tolls&quot; drive &lt;- &quot;C:/&quot; folder &lt;- &quot;data&quot; paste0(drive,folder) ## [1] &quot;C:/data&quot; The stringr package also has a str_split function that can separate a longer string using a specified split character. This is often useful: str_split(phrase,&quot; &quot;) ## [[1]] ## [1] &quot;for&quot; &quot;whom&quot; &quot;the&quot; &quot;bell&quot; &quot;tolls&quot; Example of str_c use to modify a variable needed for a join: library(tidyverse) csvPath &lt;- system.file(&quot;extdata&quot;,&quot;CA/CA_MdInc.csv&quot;,package=&quot;igisci&quot;) CA_MdInc &lt;- read_csv(csvPath) join_id &lt;- paste0(&quot;0&quot;,CA_MdInc$NAME) # could also use str_pad(CA_MdInc$NAME,1,side=&quot;left&quot;,pad=&quot;0&quot;) head(CA_MdInc) ## # A tibble: 6 × 3 ## trID NAME HHinc2016 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6001400100 60014001 177417 ## 2 6001400200 60014002 153125 ## 3 6001400300 60014003 85313 ## 4 6001400400 60014004 99539 ## 5 6001400500 60014005 83650 ## 6 6001400600 60014006 61597 head(join_id) ## [1] &quot;060014001&quot; &quot;060014002&quot; &quot;060014003&quot; &quot;060014004&quot; &quot;060014005&quot; &quot;060014006&quot; There’s a lot more to string operations. See the cheat sheet at: https://www.rstudio.com/resources/cheatsheets/. 3.6 Calling functions explicitly with :: Sometimes you need to specify the package and function name this way, for instance, if more than one package has a function of the same name. You can also use this method to call a function without having loaded its library. Due to multiple packages having certain common names (like select), it’s common to use this syntax, and you’ll find that we’ll use dplyr::select(...) throughout this book. 3.7 Exercises: Data Abstraction Exercise 3.1 Create a tibble with 20 rows of two variables norm and unif with norm created with rnorm() and unif created with runif(). Exercise 3.2 Read in “TRI/TRI_2017_CA.csv” in two ways, as a normal data frame assigned to df and as a tibble assigned to tbl. What field names result for what’s listed in the CSV as 5.1_FUGITIVE_AIR? Exercise 3.3 Use the summary function to investigate the variables in either the data.frame or tibble you just created. What type of field and what values are assigned to BIA_CODE? Exercise 3.4 Create a boxplot of body_mass_g by species from the penguins data frame in the palmerpenguins package (Horst, Hill, and Gorman 2020). Exercise 3.5 Use select, mutate, and the pipe to create a penguinMass tibble where the only original variable retained is species, but with body_mass_kg created as \\(\\frac{1}{1000}\\) the body_mass_g. The statement should start with penguinMass &lt;- penguins and use a pipe plus the other functions after that. Exercise 3.6 Now, also with penguins, create FemaleChinstaps to include only the female Chinstrap penguins. Start with FemaleChinstraps &lt;- penguins %&gt;% Exercise 3.7 Now, summarize by species groups to create mean and standard deviation variables from bill_length_mm, bill_depth_mm, flipper_length_mm, and body_mass_g. Preface the variable names with either avg. or sd. Include na.rm=T with all statistics function calls. Exercise 3.8 Sort the penguins by body_mass_g. References "],["visualization.html", "Chapter 4 Visualization 4.1 ggplot2 4.2 Summary statistical graphics 4.3 Plotting Two Variables 4.4 Exercises: Visualization", " Chapter 4 Visualization In this section, we’ll explore visualization methods in R. Visualization has been a key element of R since its inception, since visualization is central to the exploratory philosophy of the language. See https://bookdown.org/igisc/EnvDataSci/ for some examples and setting parameters in base R, but we’ll focus on ggplot2. 4.1 ggplot2 The gpplot2 package, part of the tidyverse (and installed and loaded with tidyverse)is based on the Grammar of Graphics because it provides considerable control over your graphics while remaining fairly easily readable, as long as you buy into its grammar. The ggplot2 app (and its primary function ggplot) looks at three aspects of a graph: data : where are the data coming from? geometry : what type of graph are we creating? aesthetics : what choices can we make about symbology and how do we connect symbology to data? As with other tidyverse and RStudio packages, find the ggplot2 cheat sheet at https://www.rstudio.com/resources/cheatsheets/ 4.2 Summary statistical graphics Being a language that strongly supports statistical analysis, R provides quite an array of ways of exploring and analyzing data, and graphics are an important part of that. The visualization chapter of https://bookdown.org/igisc/EnvDataSci/ includes much more on these methods, and many of these set up the statistical modeling parts of that book. Looking at summary statistics and creating graphs of our data variables will also be important for mapping and other GISc methods, so we’ll look at a few examples, but you can explore further in the larger book. 4.2.1 Plotting one variable The ggplot function provides plots of single and multiple variables, using various coordinate systems (including geographic). We’ll start with just plotting one variable, which might be continuous – where we might want to see a histogram, density plot, or dot plot – or discrete – where we might want to see something like a a bar graph, like the first example below (Figure 4.1). We’ll look at a study of Normalized Difference Vegetation Index from a transect across a montane meadow in the northern Sierra Nevada, derived from multispectral drone imagery (Davis et al. 2020). library(igisci) library(tidyverse) summary(XSptsNDVI) ## DistNtoS elevation vegetation geometry ## Min. : 0.0 Min. :1510 Length:29 Length:29 ## 1st Qu.: 37.0 1st Qu.:1510 Class :character Class :character ## Median :175.0 Median :1511 Mode :character Mode :character ## Mean :164.7 Mean :1511 ## 3rd Qu.:275.5 3rd Qu.:1511 ## Max. :298.8 Max. :1511 ## NDVIgrowing NDVIsenescent ## Min. :0.3255 Min. :0.1402 ## 1st Qu.:0.5052 1st Qu.:0.2418 ## Median :0.6169 Median :0.2817 ## Mean :0.5901 Mean :0.3662 ## 3rd Qu.:0.6768 3rd Qu.:0.5407 ## Max. :0.7683 Max. :0.7578 ggplot(XSptsNDVI, aes(vegetation)) + geom_bar() FIGURE 4.1: Simple bar graph of meadow vegetation samples 4.2.2 Histogram Histograms are very useful for looking at the distribution of continuous variables (Figure 4.2). We’ll start by using a pivot table (these will be discussed in the next chapter, on data transformation.) XSptsPheno &lt;- XSptsNDVI %&gt;% filter(vegetation != &quot;pine&quot;) %&gt;% pivot_longer(cols = starts_with(&quot;NDVI&quot;), names_to = &quot;phenology&quot;, values_to = &quot;NDVI&quot;) %&gt;% mutate(phenology = str_sub(phenology, 5, str_length(phenology))) XSptsPheno %&gt;% ggplot(aes(NDVI)) + geom_histogram(binwidth=0.05) FIGURE 4.2: Distribution of NDVI, Knuthson Meadow Histograms can be created in a couple of ways, one the conventional histogram that provides the most familiar view, for instance of the “bell curve” of a normal distribution (Figure 4.3). sierraData %&gt;% ggplot(aes(TEMPERATURE)) + geom_histogram(fill=&quot;dark green&quot;) FIGURE 4.3: Distribution of Average Monthly Temperatures, Sierra Nevada Another way is a cumulative histogram, described in https://bookdown.org/igisc/EnvDataSci/ . Another approach is a density plot, which is more of a continuous type of histogram, like the following example which also applies an alpha setting to compare two distributions: XSptsPheno %&gt;% ggplot(aes(NDVI, fill=phenology)) + geom_density(alpha=0.2) FIGURE 4.4: Comparative density plot using alpha setting 4.2.3 Other statistical graphics Some other statistical graphics that are useful are boxplots, scatter plots, trend lines, and pairs plots. Tukey boxplots provide another way of looking at the distributions of continuous variables. Typically these are stratified by a factor, such as site in the euc/oak study (Figure 4.5): ggplot(data = tidy_eucoak) + geom_boxplot(aes(x = site, y = runoff_L)) FIGURE 4.5: Boxplot of runoff by site And then as we did above, we can communicate more by coloring by tree type. Note that this is called within the aes() function (Figure 4.6). ggplot(data = tidy_eucoak) + geom_boxplot(aes(x=site, y=runoff_L, color=tree)) FIGURE 4.6: Runoff at Bay Area Sites, colored as eucalyptus and oak When we get to statistical models, the first one we’ll look at is a simple linear model. It’s often useful to display this as a trend line, and this can be done with ggplot2’s geom_smooth() function, specifying the linear model “lm” method. By default, the graph displays the standard error as a gray pattern (Figure 4.7). sierraFeb %&gt;% ggplot(aes(TEMPERATURE,ELEVATION)) + geom_point(color=&quot;blue&quot;) + geom_smooth(color=&quot;red&quot;, method=&quot;lm&quot;) FIGURE 4.7: Trend line with a linear model sierraFeb %&gt;% dplyr::select(ELEVATION:TEMPERATURE) %&gt;% pairs() FIGURE 4.8: Pairs plot for Sierra Nevada stations variables Visualizing soil CO2 data with a box plot In a study of soil CO2 in the Marble Mountains of California (Davis, Amato, and Kiefer 2001), we sampled extracted soil air (Figure 4.9) in a 11-point transect across Marble Valley in 1997 (Figure 4.10). Again, a Tukey boxplot is useful for visualization (Figure 4.11). Note that in this book you’ll often see CO2 written as CO2. These are both meant to refer to carbon dioxide, but I’ve learned that subscripts in figure headings don’t always get passed through to the LaTeX compiler for the pdf/printed version, so I’m forced to write it without the subscript. Similarly CH4 might be written as CH4, etc. The same applies often to variable names and axis labels in graphs, though there are some workarounds. FIGURE 4.9: Marble Valley, Marble Mountains Wilderness, California FIGURE 4.10: Marble Mountains soil gas sampling sites, with surface topographic features and cave passages soilCO2 &lt;- soilCO2_97 soilCO2$SITE &lt;- factor(soilCO2$SITE) # in order to make the numeric field a factor ggplot(data = soilCO2, mapping = aes(x = SITE, y = CO2pct)) + geom_boxplot() FIGURE 4.11: Visualizing soil CO2 data with a Tukey box plot 4.3 Plotting Two Variables See https://bookdown.org/igisc/EnvDataSci/ for methods of graphing two variables, either both continuous or with one discrete. 4.3.1 Color systems There’s a lot to working with color, with different color schemes needed for continuous data vs discrete values, and situations like bidirectional data. We’ll look into some basics, but the reader is recommended to learn more at sources like https://cran.r-project.org/web/packages/RColorBrewer/index.html or https://colorbrewer2.org/ or just Googling “rcolorbrewer” or “colorbrewer” or even “R colors”. 4.3.1.1 Specifying colors to use for a graphical element When a color is requested for an entire graphical element, like geom_point or geom_line, and not in the aesthetics, all feature get that color. In the following graph the same x and y values are used to display as points in blue and as lines in red (Figure 4.12). sierraFeb %&gt;% ggplot(aes(TEMPERATURE,ELEVATION)) + geom_point(color=&quot;blue&quot;) + geom_line(color=&quot;red&quot;) FIGURE 4.12: Using aesthetics settings for both points and lines Note the use of pipe to start with the data then apply ggplot. This is one approach for creating graphs, and provides a fairly straightforward way to progress from data to visualization. 4.3.1.2 Color from variable, in aesthetics If color is connected to a variable within the aes() call, a color scheme is chosen to assign either a range (for continuous) or a set of unique colors (for discrete). In this graph, color is defined inside aes, so is based on COUNTY (Figure 4.13). ggplot(data=sierraFeb) + geom_point(aes(TEMPERATURE, ELEVATION, color=COUNTY)) FIGURE 4.13: Color set within aes() Note that counties represent discrete data, and this is detected by ggplot to assign an appropriate color scheme. Continuous data will require a different palette (Figure 4.14). library(tidyverse); library(lubridate); library(igisci) sagehen &lt;- read_csv(ex(&quot;sierra/sagehen_dv.csv&quot;)) ggplot(data=sagehen, aes(x=Q, y=EC, col=waterTmax)) + geom_point() + scale_x_log10() + scale_y_log10() FIGURE 4.14: Streamflow (Q) and specific electrical conductance (EC) for Sagehen Creek, colored by temperature River map and profile We’ll build a riverData dataframe with x and y location values and elevation. We’ll need to start by creating empty vectors we’ll populate with values in a loop: d, longd, and s are assigned an empty value double(), then slope s (since it only occurs between two points) needs one NA value assigned for the last point s[length(x] to have the same length as other vectors. library(tidyverse) x &lt;- c(1000, 1100, 1300, 1500, 1600, 1800, 1900) y &lt;- c(500, 780, 820, 950, 1250, 1320, 1500) elev &lt;- c(0, 1, 2, 5, 25, 75, 150) d &lt;- double() # creates an empty numeric vector longd &lt;- double() # (&quot;double&quot; means double-precision floating point) s &lt;- double() for(i in 1:length(x)){ if(i==1){longd[i] &lt;- 0; d[i] &lt;-0} else{ d[i] &lt;- sqrt((x[i]-x[i-1])^2 + (y[i]-y[i-1])^2) longd[i] &lt;- longd[i-1] + d[i] s[i-1] &lt;- (elev[i]-elev[i-1])/d[i]}} s[length(x)] &lt;- NA # make the last slope value NA since we have no data past it, # and so the vector lengths are all the same riverData &lt;- bind_cols(x=x,y=y,elev=elev,d=d,longd=longd,s=s) riverData ## # A tibble: 7 × 6 ## x y elev d longd s ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1000 500 0 0 0 0.00336 ## 2 1100 780 1 297. 297. 0.00490 ## 3 1300 820 2 204. 501. 0.0126 ## 4 1500 950 5 239. 740. 0.0632 ## 5 1600 1250 25 316. 1056. 0.236 ## 6 1800 1320 75 212. 1268. 0.364 ## 7 1900 1500 150 206. 1474. NA For this continuous data, a range of values is detected and a continous color scheme is assigned (Figure 4.15). The ggplot scale_color_gradient function is used to establish end points of a color range that the data are stretched between (Figure 4.16). We can use this for many continuous variables, such as slope (Figure 4.17). The scale_color_gradient2 lets you use a mid color. Note that there’s a comparable scale_fill_gradient and scale_fill_gradient2 for use when specifying a fill (e.g. for a polygon) instead of a color (for polygons linked to the border). ggplot(riverData, aes(x,y)) + geom_line(mapping=aes(col=s), size=1.2) + geom_point(mapping=aes(col=s, size=elev)) + coord_fixed(ratio=1) + scale_color_gradient(low=&quot;green&quot;, high=&quot;red&quot;) + ggtitle(&quot;Simulated river path, elevations, and slopes&quot;) FIGURE 4.15: Channel slope as range from green to red, vertices sized by elevation ggplot(riverData, aes(longd,elev)) + geom_line(aes(col=s), size=1.5) + geom_point() + scale_color_gradient(low=&quot;green&quot;, high=&quot;red&quot;) + ggtitle(&quot;Longitudinal profile&quot;) FIGURE 4.16: Channel slope as range of line colors on a longitudinal profile ggplot(riverData, aes(longd,s)) + geom_point(aes(col=s), size=3) + scale_color_gradient(low=&quot;green&quot;, high=&quot;red&quot;) + ggtitle(&quot;Slope by longitudinal distance upstream&quot;) FIGURE 4.17: Channel slope by longitudinal distance as scatter points colored by slope 4.4 Exercises: Visualization Exercise 4.1 Create a bar graph of the counts of the species in the penguins data frame (Horst, Hill, and Gorman 2020). What can you say about what it shows? Exercise 4.2 Use bind_cols in dplyr to create a tibble from built-in vectors state.abb and state.region, then use ggplot with geom_bar to create a bar graph of the four regions. Exercise 4.3 Convert the built-in time series treering into a tibble tr using the tibble() function with the single variable assigned as treering = treering (or just specifying treering will also work for this simple example), then create a histogram, using that tibble and variable for the data and x settings needed. Attach a screen capture of the histogram. (Also, learn about the treering data by entering ?treering in the console and read the Help displayed.) Exercise 4.4 Create a new tibble st using bind_cols with Name=state.name, Abb=state.abb, Region=state.region, and as_tibble(state.x77). Note that this works since all of the parts are sorted by state. From st, create a density plot from the variable Frost (number of days with frost for that state). What is the approximate modal value? Exercise 4.5 From st create a a boxplot of Area by Region. Which region has the highest and which has the lowest median Area? Do the same for Frost. Exercise 4.6 From st, compare murder rate (y is Murder) to Frost (as x) in a scatter plot, colored by Region. Exercise 4.7 Add a trend line (smooth) with method=“lm” to your scatterplot, not colored by Region (but keep the points colored by region). What can you say about what this graph is showing you? References "],["spatial.html", "Chapter 5 Spatial Data and Maps 5.1 Spatial Data 5.2 Coordinate Referencing Systems 5.3 Creating sf Data from Data Frames 5.4 Building a 2-state map from geometry 5.5 Using maptiles to create a basemap 5.6 Raster data 5.7 ggplot2 for Maps 5.8 tmap 5.9 Interactive Maps 5.10 Exercises: Spatial Data and Maps", " Chapter 5 Spatial Data and Maps The Spatial section of this book adds the spatial dimension and geographic data science. Most environmental systems are significantly affected by location, so the geographic perspective is highly informative. In this chapter, we’ll explore the basics of building and using feature-based (vector) data using the sf (simple features) package raster data using the terra package some common mapping methods in plot and ggplot2 Especially for the feature data, which is built on a database model (with geometry as one variable), the tools we’ve been learning in dplyr and tidyr will also be useful to working with attributes. Caveat: Spatial data and analysis is very useful for environmental data analysis, and we’ll only scratch the surface. Readers are encouraged to explore other resources on this important topic, such as: Geocomputation with R at https://geocompr.robinlovelace.net/ Simple Features for R at https://r-spatial.github.io/sf/ Spatial Data Science with R at https://rspatial.org 5.1 Spatial Data Spatial data are data using a Cartesian coordinate system with x, y, z, and maybe more dimensions. Geospatial data are spatial data that we can map on our planet and relate to other geospatial data based on geographic coordinate systems (GCS) of longitude and latitude or known projections to planar coordinates like Mercator, Albers, or many others. You can also use local coordinate systems with the methods we’ll learn, to describe geometric objects or a local coordinate system to “map out” an archaeological dig or describe the movement of ants on an anthill, but we’ll primarily use geospatial data. To work with spatial data requires extending R to deal with it using packages. Many have been developed, but the field is starting to mature using international open GIS standards. We’ll look at two, one for features, the other including rasters, and you’ll need to install each with install.packages(\"sf\") and install.packages(\"terra\"). sf (Simple Features) Feature-based (vector) methods ISO 19125 standard for GIS geometries Has functions for working with spatial data Doesn’t need many additional packages, though you may still need rgdal installed for some tools you want to use Works with ggplot2 and tmap for nice looking maps Cheat sheet and other information at https://r-spatial.github.io/sf/ Earliest CRAN archive is 0.2 on 2016-10-26, 1.0 not until 2021-06-29 Author: Edzer Pebesma terra Intended to eventually replace raster, by the same author (Hijmans) CRAN: “Methods for spatial data analysis with raster and vector data. Raster methods allow for low-level data manipulation as well as high-level global, local, zonal, and focal computation. The predict and interpolate methods facilitate the use of regression type (interpolation, machine learning) models for spatial prediction, including with satellite remote sensing data. Processing of very large files is supported. See the manual and tutorials on https://rspatial.org/terra/ to get started.” 5.1.1 Simple geometry building in sf and terra We won’t explore this here, but the Introduction to Environmental Data Science book https://bookdown.org/igisc/EnvDataSci/ includes methods for building sf and terra features from geometries. We’ll skip to using points in data frames and shapefiles. 5.1.2 Building points from a data frame We’re going to take a different approach with points, since if we’re building them from scratch they commonly come in the form of a data frame with x and y columns, so the st_as_sf is a nice simple approach to bringing in points with coordinates stored in a data frame, a very common situation. The sf::st_as_sf function is useful for a lot of things, and follows the convention of R’s many “as” functions: if it can understand the input as having “space and time” (st) data, it can convert it to an sf. We’ll start by creating a data frame representing 12 Sierra Nevada (and some adjacent) weather stations, with variables entered as vectors, including attributes. The order of vectors needs to be the same for all, where the first point has index 1, etc. sta &lt;- c(&quot;OROVILLE&quot;,&quot;AUBURN&quot;,&quot;PLACERVILLE&quot;,&quot;COLFAX&quot;,&quot;NEVADA CITY&quot;,&quot;QUINCY&quot;, &quot;YOSEMITE&quot;,&quot;PORTOLA&quot;,&quot;TRUCKEE&quot;,&quot;BRIDGEPORT&quot;,&quot;LEE VINING&quot;,&quot;BODIE&quot;) long &lt;- c(-121.55,-121.08,-120.82,-120.95,-121.00,-120.95, -119.59,-120.47,-120.17,-119.23,-119.12,-119.01) lat &lt;- c(39.52,38.91,38.70,39.09,39.25,39.94, 37.75,39.81,39.33,38.26,37.96,38.21) elev &lt;- c(52,394,564,725,848,1042, 1225,1478,1775,1972,2072,2551) temp &lt;- c(10.7,9.7,9.2,7.3,6.7,4.0, 5.0,0.5,-1.1,-2.2,0.4,-4.4) prec &lt;- c(124,160,171,207,268,182, 169,98,126,41,72,40) To build the sf data, we start by building the data frame… df &lt;- data.frame(sta,elev,temp,prec,long,lat) … then use the very handy sf::st_as_sf function to make an sf out of it, using the long and lat as a source of coordinates (Figure 5.1). wst &lt;- st_as_sf(df, coords=c(&quot;long&quot;,&quot;lat&quot;), crs=4326) ggplot(data=wst) + geom_sf(mapping = aes(col=elev)) FIGURE 5.1: Points created from a dataframe with Simple Features 5.1.3 Creating features from shapefiles Both sf’s st_read and terra’s vect read the open-GIS shapefile format developed by Esri for points, polylines, and polygons. You would normally have shapefiles (and all the files that go with them – .shx, etc.) stored on your computer, but we’ll access one from the igisci external data folder, and use that ex() function we used earlier with CSVs. Remember that we could just include that and the library calls just once at the top of our code like this… library(igisci) library(sf) If we just send a spatial dataset like an sf spatial data frame to the plot system, it will plot all of the variables by default (Figure 5.2). BayAreaCounties &lt;- st_read(ex(&quot;BayArea/BayAreaCounties.shp&quot;)) plot(BayAreaCounties) FIGURE 5.2: A simple plot of polygon data by default shows all variables But with just one variable, of course, it just produces a single map (Figure 5.3). plot(BayAreaCounties[&quot;POPULATION&quot;]) FIGURE 5.3: A single map with a legend is produced when a variable is specified Notice that in the above map, we used the [] accessor. Why didn’t we use the simple $ accessor? Remember that plot() figures out what to do based on what you provide it. And there’s an important difference in what you get with the two accessors, which we can check with class(): class(BayAreaCounties[&quot;POPULATION&quot;]) ## [1] &quot;sf&quot; &quot;data.frame&quot; class(BayAreaCounties$POPULATION) ## [1] &quot;numeric&quot; You might see that what you get with plot(BayAreaCounties$POPULATION) is not very informative, since the object is just a numeric vector, while using [] accessor returns a spatial dataframe. There’s a lot more we could do with the base R plot system, so we’ll learn some of these before exploring what we can do with ggplot2, tmap, and leaflet. But first we need to learn more about building geospatial data. We’ll use st_as_sf() for that, but we’ll need to specify the coordinate referencing system (CRS), in this case GCS. We’ll only briefly explore how to specify the CRS here. For a thorough coverage, please see Lovelace, Nowosad, and Muenchow (2019). 5.2 Coordinate Referencing Systems Before we try the next method for bringing in spatial data – converting data frames – we need to look at coordinate referencing systems (CRS). First, there are quite a few, with some spherical like the geographic coordinate system (GCS) of longitude and latitude, and others planar projections of GCS using mathematically defined projections such as Mercator, Albers Conformal Conic, Azimuthal, etc., and including widely used government-developed systems such as UTM (universal transverse mercator) or state plane. Even for GCS, there are many varieties since geodetic datums can be chosen, and for very fine resolution work where centimetres or even millimetres matter, this decision can be important (and tectonic plate movement can play havoc with tying it down.) There are also multiple ways of expressing the CRS, either to read it or to set it. The full specification of a CRS can be displayed for data already in a CRS, with either sf::st_crs or terra::crs. st_crs(CA_counties) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] … though there are other ways to see the crs in shorter forms, or its individual properties : st_crs(CA_counties)$proj4string ## [1] &quot;+proj=longlat +datum=WGS84 +no_defs&quot; st_crs(CA_counties)$units_gdal ## [1] &quot;degree&quot; st_crs(CA_counties)$epsg ## [1] 4326 So, to convert the sierra data into geospatial data with st_as_sf, we might either do it with the reasonably short PROJ format … GCS &lt;- &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; wsta = st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;,&quot;LATITUDE&quot;), crs=GCS) … or with the even shorter EPSG code (which we can find by Googling), which can either be provided as text \"epsg:4326\" or often just the number, which we’ll use next. 5.3 Creating sf Data from Data Frames As we saw earlier, if your data frame has geospatial coordinates like LONGITUDE and LATITUDE … names(sierraFeb) ## [1] &quot;STATION_NAME&quot; &quot;COUNTY&quot; &quot;ELEVATION&quot; &quot;LATITUDE&quot; ## [5] &quot;LONGITUDE&quot; &quot;PRECIPITATION&quot; &quot;TEMPERATURE&quot; … we have what we need to create geospatial data from it. Earlier we read in a series of vectors built in code with c() functions from 12 selected weather stations; this time we’ll use a data frame that has all of the Sierra Nevada weather stations (Figure 5.4). wsta &lt;- st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;,&quot;LATITUDE&quot;), crs=4326) ggplot(data=wsta) + geom_sf(aes(col=ELEVATION)) FIGURE 5.4: Points created from data frame with coordinate variables 5.4 Building a 2-state map from geometry Let’s briefly look at some code that builds two state boundary features with attributes from a set of GCS coordinates entered as vectors of coordinate pairs, using sf geometry-building methods, with simple attribues added using bind_rows(). Feel free to just copy this and run it. We aren’t going to go over these methods in this 2-day course, but building geometries this way is covered in https://bookdown.org/igisc/EnvDataSci/. library(sf); library(tidyverse) CA_matrix &lt;- rbind(c(-124,42),c(-120,42),c(-120,39),c(-114.5,35), c(-114.1,34.3),c(-114.6,32.7),c(-117,32.5),c(-118.5,34),c(-120.5,34.5), c(-122,36.5),c(-121.8,36.8),c(-122,37),c(-122.4,37.3),c(-122.5,37.8), c(-123,38),c(-123.7,39),c(-124,40),c(-124.4,40.5),c(-124,41),c(-124,42)) NV_matrix &lt;- rbind(c(-120,42),c(-114,42),c(-114,36),c(-114.5,36), c(-114.5,35),c(-120,39),c(-120,42)) CA_list &lt;- list(CA_matrix); NV_list &lt;- list(NV_matrix) CA_poly &lt;- st_polygon(CA_list); NV_poly &lt;- st_polygon(NV_list) sfc_2states &lt;- st_sfc(CA_poly,NV_poly,crs=4326) # crs=4326 specifies GCS # st_geometry_type(sfc_2states) attributes &lt;- bind_rows(c(abb=&quot;CA&quot;, area=423970, pop=39.56e6), c(abb=&quot;NV&quot;, area=286382, pop=3.03e6)) %&gt;% mutate(area=as.numeric(area),pop=as.numeric(pop)) twostates &lt;- st_sf(attributes, geometry = sfc_2states) 5.5 Using maptiles to create a basemap For vector data, it’s often nice to display over a basemap by accessing raster tiles that are served on the internet by various providers. We’ll use the maptiles package to try displaying the CA and NV boundaries we created earlier. The maptiles package supports a variety of basemap providers, and I’ve gotten the following to work: \"OpenStreetMap\", \"Stamen.Terrain\", \"Stamen.TerrainBackground\", \"Esri.WorldShadedRelief\", \"Esri.NatGeoWorldMap\", \"Esri.WorldGrayCanvas\", \"CartoDB.Positron\", \"CartoDB.Voyager\", \"CartoDB.DarkMatter\", \"OpenTopoMap\", \"Wikimedia\", however, they don’t work at all scales and locations – you’ll often see an Error in grDevices, if so then try another provider – the default \"OpenStreetMap\" seems to work the most reliably (Figure 5.5). library(terra); library(maptiles) # Get the raster that covers the extent of CANV: calnevaBase &lt;- get_tiles(twostates, provider=&quot;OpenTopoMap&quot;) st_crs(twostates)$epsg ## [1] 4326 plotRGB(calnevaBase) # starts plot with a raster basemap lines(vect(twostates), col=&quot;black&quot;, lwd=2) FIGURE 5.5: Using maptiles for a base map In the code above, we can also see the use of terra functions and parameters. Learn more about these by reviewing the code and considering: terra functions: The terra::vect() function creates a SpatVector that works with terra::lines to display the boundary lines in plot(). (More on terra later, in the Raster section.) parameters: Note the use of the parameter lwd (line width) from the plot system. This is one of many parameter settings described in ?par. It defaults to 1, so 2 makes it twice as thick. You could also use lty (line type) to change it to a dashed line with lty=\"dashed\" or lty=2. And for the sierraFeb data, we’ll start with st_as_sf and the coordinate system (4326 for GCS), then use a maptiles basemap again, and the terra::points method to add the points (Figure 5.6). library(terra); library(maptiles) sierraFebpts &lt;- st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) sierraBase &lt;- get_tiles(sierraFebpts) st_crs(sierraFebpts)$epsg ## [1] 4326 plotRGB(sierraBase) points(vect(sierraFebpts)) FIGURE 5.6: Converted sf data for map with tiles 5.6 Raster data Simple Features are feature-based, of course, so it’s not surprising that sf doesn’t have support for rasters. So we’ll want to either use the raster package or its imminent replacement, terra (which also has vector methods – see Hijmans (n.d.) and Lovelace, Nowosad, and Muenchow (2019)), and I’m increasingly using terra since it has some improvements that I find useful. 5.6.1 Building rasters We can start by building one from scratch. More explanation on this method is in https://bookdown.org/igisc/EnvDataSci/, but here’s a quick look. The terra package has a rast() function for this. library(terra) new_ras &lt;- rast(ncol = 12, nrow = 6, xmin = -180, xmax = 180, ymin = -90, ymax = 90, vals = 1:72) names(new_ras) &lt;- &quot;world30deg&quot; Simple feature data or SpatVectors can be plotted along with rasters using terra’s lines, polys, or points functions. Here we’ll use the twostates sf we created earlier (make sure to run that code again if you need it). Look closely at the map for our two states (Figure 5.7). plot(new_ras, main=paste(&quot;CANV on&quot;,new_ras@ptr$names)) CANV &lt;- vect(twostates) lines(CANV) FIGURE 5.7: Simple plot of a worldwide SpatRaster of 30-degree cells, with SpatVector of CA and NV added 5.6.2 Vector to raster conversion To convert rasters to vectors requires having a template raster with the desired cell size and extent, or an existing raster we can use as a template – we ignore what the values are – such as elev.tif in the following example (Figure 5.8): streams &lt;- vect(ex(&quot;marbles/streams.shp&quot;)) elev &lt;- rast(ex(&quot;marbles/elev.tif&quot;)) streamras &lt;- rasterize(streams,elev) plot(streamras, col=&quot;blue&quot;) FIGURE 5.8: Stream raster converted from stream features, with 30 m cells from an elevation raster template 5.6.2.1 What if we have no raster to use as a template? There’s a way. See the coverage of this in https://bookdown.org/igisc/EnvDataSci/ . 5.6.2.2 Plotting some existing downloaded raster data Let’s plot some Shuttle Radar Topography Mission (SRTM) elevation data for the Virgin River Canyon at Zion National Park (Figure 5.9), from Jakub Nowosad’s spDataLarge repository, which we’ll need to first install with: install.packages(&quot;spDataLarge&quot;,repos=&quot;https://nowosad.github.io/drat/&quot;,type=&quot;source&quot;) library(terra) plot(rast(system.file(&quot;raster/srtm.tif&quot;, package=&quot;spDataLarge&quot;))) FIGURE 5.9: Shuttle Radar Topography Mission (SRTM) image of Virgin River Canyon area, southern Utah 5.7 ggplot2 for Maps The Grammar of Graphics is the gg of ggplot. Key concept is separating aesthetics from data Aesthetics can come from variables (using aes()setting) or be constant for the graph Mapping tools that follow this lead ggplot, as we have seen, and it continues to be enhanced (Figure 5.10) tmap (Thematic Maps) https://github.com/mtennekes/tmap Tennekes, M., 2018, tmap: Thematic Maps in R, Journal of Statistical Software 84(6), 1-39 ggplot(CA_counties) + geom_sf() FIGURE 5.10: simple ggplot map Try ?geom_sf and you’ll find that its first parameter is mapping with aes() by default. The data property is inherited from the ggplot call, but commonly you’ll want to specify data=something in your geom_sf call. Another simple ggplot, with labels Adding labels is also pretty easy using aes() (Figure 5.11). ggplot(CA_counties) + geom_sf() + geom_sf_text(aes(label = NAME), size = 1.5) FIGURE 5.11: labels added And now with fill color, repositioned legend, and no “x” or “y” labels The x and y labels are unnecessary since the graticule is provided, and for many maps there’s a better place to put the legend than what happens by default – for California’s shape, the legend goes best in Nevada (Figure 5.12). ggplot(CA_counties) + geom_sf(aes(fill=MED_AGE)) + geom_sf_text(aes(label = NAME), col=&quot;white&quot;, size=1.5) + theme(legend.position = c(0.8, 0.8)) + labs(x=&quot;&quot;,y=&quot;&quot;) FIGURE 5.12: repositioned legend Map in ggplot2, zoomed into two counties We can zoom into two counties by accessing the extent of an existing spatial dataset using st_bbox() (Figure 5.13). library(tidyverse); library(sf); library(igisci) census &lt;- st_make_valid(BayAreaTracts) %&gt;% filter(CNTY_FIPS %in% c(&quot;013&quot;, &quot;095&quot;)) TRI &lt;- read_csv(ex(&quot;TRI/TRI_2017_CA.csv&quot;)) %&gt;% st_as_sf(coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) %&gt;% st_join(census) %&gt;% filter(CNTY_FIPS %in% c(&quot;013&quot;, &quot;095&quot;), (`5.1_FUGITIVE_AIR` + `5.2_STACK_AIR`) &gt; 0) bnd = st_bbox(census) ggplot() + geom_sf(data = BayAreaCounties, aes(fill = NAME)) + geom_sf(data = census, color=&quot;grey40&quot;, fill = NA) + geom_sf(data = TRI) + coord_sf(xlim = c(bnd[1], bnd[3]), ylim = c(bnd[2], bnd[4])) + labs(title=&quot;Census Tracts and TRI air-release sites&quot;) + theme(legend.position = &quot;none&quot;) FIGURE 5.13: Using bbox to zoom into two counties 5.8 tmap The tmap package provides some nice cartographic capabilities. We’ll use some of its capabilities, but for more thorough coverage, see the “Making maps with R” section of Geocomputation with R (Lovelace, Nowosad, and Muenchow (2019)). The basic building block is tm_shape(data) followed by various layer elements, such as tm_fill(). The tm_shape function can work with either features or rasters. Rasters can be displayed in ggplot2, however it’s a bit clumsy so I prefer tmap when you’re using rasters. library(spData); library(tmap) m &lt;- tm_shape(world) + tm_fill() + tm_borders() But we’ll make a better map (Figure 5.14) by inserting a graticule before filling the polygons, then use tm_layout for some useful cartographic settings: make the background light blue and avoid excessive inner margins. library(spData); library(tmap) bounds &lt;- st_bbox(world) m &lt;- tm_shape(world, bbox=bounds) + tm_graticules(col=&quot;seashell2&quot;) + tm_fill() + tm_borders() + tm_layout(bg.color=&quot;lightblue&quot;, inner.margins=0) m FIGURE 5.14: tmap of the world Experiment by leaving settings out to see the effect, and explore other options with ?tm_layout, etc. The tmap package has a wealth of options, just a few of which we’ll explore. For instance, we might want to use a different map projection than the default. Color by variable As with plot and ggplot2, we can reference a variable to provide a range of colors for features we’re plotting, such as coloring polygon fills to create a choropleth map (Figure 5.15). library(sf); library(igisci) library(tmap) tm_shape(st_make_valid(BayAreaTracts)) + tm_graticules(col=&quot;#e5e7e9&quot;) + tm_fill(col = &quot;MED_AGE&quot;) FIGURE 5.15: tmap fill colored by variable tmap of sierraFeb with hillshade and point symbols We’ll use a raster hillshade as a basemap using tm_raster, zoom into an area with a bounding box (from st_bbox), include county boundaries with tm_borders, and color station points with temperatures with tm_symbols (Figure 5.16). library(terra) tmap_mode(&quot;plot&quot;) tmap_options(max.categories = 8) sierra &lt;- st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs = 4326) hillsh &lt;- rast(ex(&quot;CA/ca_hillsh_WGS84.tif&quot;)) # alt: hillsh &lt;- raster(...) bounds &lt;- st_bbox(sierra) tm_shape(hillsh,bbox=bounds)+ tm_graticules(col=&quot;azure2&quot;) + tm_raster(palette=&quot;-Greys&quot;,legend.show=FALSE,n=10) + tm_shape(sierra) + tm_symbols(col=&quot;TEMPERATURE&quot;, palette=c(&quot;blue&quot;,&quot;red&quot;), style=&quot;cont&quot;,n=8) + tm_shape(st_make_valid(CA_counties)) + tm_borders() + tm_legend() + tm_layout(legend.position=c(&quot;RIGHT&quot;,&quot;TOP&quot;)) FIGURE 5.16: hillshade, borders and point symbols in tmap Color (rgb) basemap Earlier we looked at creating a basemap using maptiles. These are also supported in tmap using the tm_rgb (Figure 5.17). Note that here the graticule is covered by the raster tiles, as it was by polygons earlier. We could have put it after the basemap tiles, but it doesn’t look good, but works ok just showing on the edges. library(tmap); library(maptiles) # The following gets the raster that covers the extent of CA &amp; NV [twostates] calnevaBase &lt;- get_tiles(twostates, provider=&quot;OpenTopoMap&quot;)# g tm_shape(calnevaBase) + tm_graticules() + tm_rgb() + tm_shape(twostates) + tm_borders(lwd=3) FIGURE 5.17: Two western states with a basemap in tmap 5.9 Interactive Maps The word “static” in “static maps” isn’t something you would have heard in a cartography class thirty years ago, since essentially all maps then were static. Very important in designing maps was, and still is, considering your audience and their perception of symbology. Figure-to-ground relationships assume “ground” is a white piece of paper (or possibly a standard white background in a pdf), so good cartographic color schemes tend to range from light for low values to dark for high values. Scale is fixed, and there are no “tools” for changing scale, so a lot of attention must be paid to providing scale information. Similarly, without the ability to see the map at different scales, inset maps are often needed to provide context. Interactive maps change the game in having tools for changing scale and always being “printed” on a computer or device where the color of the background isn’t necessarily white. We are increasingly used to using interactive maps on our phones or other devices, and often get frustrated not being able to zoom into a static map. However, as we’ll see, there are trade-offs in that interactive maps don’t provide the same level of control on symbology for what we want to put on the map, but instead depend a lot on basemaps for much of the cartographic design, generally limiting the symbology of data being mapped on top of it. 5.9.1 Leaflet We’ll come back to tmap to look at its interactive option, but we should start with a very brief look at the package that it uses when you choose interactive mode: leaflet. The R leaflet library itself translates to Javascript and its Leaflet library, which was designed to support “mobile-friendly interactive maps” (https://leafletjs.com). We’ll also look at another R package that translates to leaflet: mapview. Interactive maps tend to focus on using basemaps for most of the cartographic design work, and quite a few are available. The only code required to display the basemap is addTiles(), which will display the default OpenStreetMap in the area where you provide any features, typically with addMarkers(). This default basemap is pretty good to use for general purposes, especially in urban areas where OpenStreetMap contributors (including a lot of former students I think) have provided a lot of data. You can specify additional choices using addProviderTiles(), and use a layers control with the choices provided as baseGroups. You have access to all of the basemap provider names with the vector providers, and to see what they look like in your area, explore http://leaflet-extras.github.io/leaflet-providers/preview/index.html, where you can zoom into an area of interest and select the base map to see how it would look in a given zoom (Figure 5.18). library(leaflet) leaflet() %&gt;% addTiles(group=&quot;OpenStreetMap&quot;) %&gt;% addProviderTiles(&quot;OpenTopoMap&quot;,group=&quot;OpenTopoMap&quot;) %&gt;% addProviderTiles(&quot;Esri.NatGeoWorldMap&quot;,group=&quot;Esri.NatGeoWorldMap&quot;) %&gt;% addProviderTiles(&quot;Esri.WorldImagery&quot;,group=&quot;Esri.WorldImagery&quot;) %&gt;% addMarkers(lng=-122.4756, lat=37.72222, popup=&quot;Institute for Geographic Information Science, SFSU&quot;) %&gt;% addLayersControl( baseGroups = c(&quot;OpenStreetMap&quot;,&quot;OpenTopoMap&quot;, &quot;Esri.WorldImagery&quot;,&quot;Esri.NatGeoWorldMap&quot;)) FIGURE 5.18: Leaflet map showing the location of the SFSU Institute for Geographic Information Science with choices of basemaps With an interactive map, we do have the advantage of a good choice of base maps and the ability to resize and explore the map, but symbology is more limited, mostly just color and size, with only one variable in a legend. Note that interactive maps display in the Viewer window of RStudio or in R Markdown code output as you see in the html version of this book. For a lot more information on the leaflet package in R, see: https://blog.rstudio.com/2015/06/24/leaflet-interactive-web-maps-with-r/ https://github.com/rstudio/cheatsheets/blob/master/leaflet.pdf 5.9.2 tmap (view mode) You can change to an interactive mode with tmap by using tmap_mode(\"view\") so you might think that you can do all the great things that tmap does in normal plot mode here, but as we’ve seen, interactive maps don’t provide the same level of symbology. The view mode of tmap, like mapview, is just a wrapper around leaflet, so we’ll focus on the latter. The key parameter needed is tmap_mode, which must be set to \"view\" to create an interactive map. library(tmap); library(sf) tmap_mode(&quot;view&quot;) tmap_options(max.categories = 8) sierra &lt;- st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs = 4326) bounds &lt;- st_bbox(sierra) #sierraMap &lt;- tm_basemap(leaflet::providers$Esri.NatGeoWorldMap) + tm_shape(sierra) + tm_symbols(col=&quot;TEMPERATURE&quot;, palette=c(&quot;blue&quot;,&quot;red&quot;), style=&quot;cont&quot;,n=8,size=0.2) + tm_legend() + tm_layout(legend.position=c(&quot;RIGHT&quot;,&quot;TOP&quot;)) #sierraMap #tmap_leaflet(sierraMap) One nice feature of tmap view mode (and mapview) is the ability to select the basemap interactively using the layers symbol. There are lots of basemap available with leaflet (and thus with tmap view). To explore them, see http://leaflet-extras.github.io/leaflet-providers/preview/index.html but recognize that not all of these will work everywhere; many are highly localized or may only work at certain scales. The following map using tmap could presumably be coded in leaflet, but tmap makes it a lot easier. We’ll use it to demonstrate picking the basemap, which really can help in communicating our data with context (Figure ??). library(sf); library(tmap); library(igisci) tmap_mode = &quot;view&quot; tmap_options(basemaps=c(Topo=&quot;OpenTopoMap&quot;, Imagery = &quot;Esri.WorldImagery&quot;, NatGeo=&quot;Esri.NatGeoWorldMap&quot;)) soilCO2 &lt;- st_read(ex(&quot;marbles/co2july95.shp&quot;)) geology &lt;- st_read(ex(&quot;marbles/geology.shp&quot;)) mblCO2map &lt;- tm_basemap() + tm_shape(geology) + tm_fill(col=&quot;CLASS&quot;, alpha=0.5) + tm_shape(soilCO2) + tm_symbols(col=&quot;CO2_&quot;, palette=&quot;viridis&quot;, style=&quot;cont&quot;,n=8,size=0.6) + tm_legend() + tm_layout(legend.position=c(&quot;RIGHT&quot;,&quot;TOP&quot;)) mblCO2map 5.9.3 Interactive mapping of individual penguins abstracted from a big dataset In a study of spatial foraging patterns by Adélie penguins (Pygoscelis adeliae), Ballard et al. (2019) collected data over five austral summer seasons (parts of December and January in 2005-06, 2006-07, 2007-08, 2008-09, 2012-13) period using SPLASH tags that combine Argos satellite tracking with a time-depth recorder, with 11,192 observations. The 24 SPLASH tags were re-used over the period of the project with a total of 162 individual breeding penguins, each active over a single season. Our code takes advantage of the abstraction capabilities of base R and dplyr to select an individual penguin from the large data set and prepare its variables for useful display. Then the interactive mode of tmap works well for visualizing the penguin data – both the cloud of all observations and the focused individual – since we can zoom in to see the locations in context, and basemaps can be chosen. The tmap code colors the individual penguin’s locations based on a decimal day derived from day and time. While interactive view is more limited in options, it does at least support a legend and title (Figure 5.19). library(sf); library(tmap); library(tidyverse); library(igisci) sat_table &lt;- read_csv(ex(&quot;penguins/sat_table_splash.csv&quot;)) obs &lt;- st_as_sf(filter(sat_table,include==1), coords=c(&quot;lon&quot;,&quot;lat&quot;), crs=4326) uniq_ids &lt;- unique(obs$uniq_id) # uniq_id identifies an individual penguin onebird &lt;- obs %&gt;% filter(uniq_id==uniq_ids[sample.int(length(uniq_ids), size=1)]) %&gt;% mutate(decimalDay = as.numeric(day) + as.numeric(hour)/24) tmap_mode = &quot;view&quot; tmap_options(basemaps=c(Terrain = &quot;Esri.WorldTerrain&quot;, Imagery = &quot;Esri.WorldImagery&quot;, OceanBasemap = &quot;Esri.OceanBasemap&quot;, Topo=&quot;OpenTopoMap&quot;, Ortho=&quot;GeoportailFrance.orthos&quot;)) penguinMap &lt;- tm_basemap() + tm_shape(obs) + tm_symbols(col=&quot;gray&quot;, size=0.01, alpha=0.5) + tm_shape(onebird) + tm_symbols(col=&quot;decimalDay&quot;, palette=&quot;viridis&quot;, style=&quot;cont&quot;,n=8,size=0.6) + tm_legend() + tm_layout() tmap_leaflet(penguinMap) FIGURE 5.19: Observations of Adélie penguin migration from a 5-season study of a large colony at Ross Island in the SW Ross Sea, Antarctica; and an individual – H36CROZ0708 – from season 0708. Data source: Ballard et al. (2019). Fine-scale oceanographic features characterizing successful Adélie penguin foraging in the SW Ross Sea. Marine Ecology Progress Series 608:263-277. Some final thoughts on maps and plotting/viewing packages: There’s a lot more to creating maps from spatial data, but we need to look at spatial analysis to create some products that we might want to create maps from. In the next chapter, in addition to looking at spatial analysis methods we’ll also continue our exploration of map design, ranging from very simple exploratory outputs to more carefully designed products for sharing with others. 5.9.4 Exercise project preparation For the exercises, you might want to create a new RStudio project, and name it “Spatial”. We’re going to use this for data we create and new data we want to bring in. We’ll still be reading in data from the data package, but working in this project we’ll be getting used to (a) working with our own data and (b) storing data to be used for later projects. Once we’ve created the project, we’ll want to create a data folder to store data in. The code below includes this folder creation. Go ahead and just run these bits that build the western states data and save it as a shapefile in a data folder. If you have time, figure out what it’s all doing, and refer to https://bookdown.org/igisc/EnvDataSci/spatial.html for more information. library(tidyverse); library(sf) CO &lt;- st_polygon(list(rbind(c(-109,41),c(-102,41),c(-102,37),c(-109,37),c(-109,41)))) WY &lt;- st_polygon(list(rbind(c(-111,45),c(-104,45),c(-104,41),c(-111,41),c(-111,45)))) UT &lt;- st_polygon(list(rbind(c(-114,42),c(-111,42),c(-111,41),c(-109,41),c(-109,37), c(-114,37),c(-114,42)))) AZ &lt;- st_polygon(list(rbind(c(-114,37),c(-109,37),c(-109,31.3),c(-111,31.3), c(-114.8,32.5),c(-114.6,32.7),c(-114.1,34.3),c(-114.5,35), c(-114.5,36),c(-114,36),c(-114,37)))) NM &lt;- st_polygon(list(rbind(c(-109,37),c(-103,37),c(-103,32),c(-106.6,32), c(-106.5,31.8),c(-108.2,31.8),c(-108.2,31.3),c(-109,31.3),c(-109,37)))) CA &lt;- st_polygon(list(rbind(c(-124,42),c(-120,42),c(-120,39),c(-114.5,35), c(-114.1,34.3),c(-114.6,32.7),c(-117,32.5),c(-118.5,34),c(-120.5,34.5), c(-122,36.5),c(-121.8,36.8),c(-122,37),c(-122.4,37.3),c(-122.5,37.8), c(-123,38),c(-123.7,39),c(-124,40),c(-124.4,40.5),c(-124,41),c(-124,42)))) NV &lt;- st_polygon(list(rbind(c(-120,42),c(-114,42),c(-114,36),c(-114.5,36), c(-114.5,35),c(-120,39),c(-120,42)))) OR &lt;- st_polygon(list(rbind(c(-124,42),c(-124.5,43), c(-124,46),c(-123,46),c(-122.7,45.5),c(-119,46),c(-117,46), c(-116.5,45.5),c(-117.2,44.5),c(-117,44), c(-117,42),c(-120,42),c(-124,42)))) WA &lt;- st_polygon(list(rbind(c(-124,46),c(-124.8,48.4),c(-123,48), c(-123,49),c(-117,49), c(-117,46),c(-119,46),c(-122.7,45.5),c(-123,46),c(-124,46)))) ID &lt;- st_polygon(list(rbind(c(-117,49), c(-116,49),c(-116,48),c(-114.4,46.5),c(-114.4,45.5), c(-114,45.6),c(-113,44.5),c(-111,44.5), c(-111,42),c(-114,42),c(-117,42), c(-117,44),c(-117.2,44.5),c(-116.5,45.5), c(-117,46),c(-117,49)))) MT &lt;- st_polygon(list(rbind(c(-116,49),c(-104,49), c(-104,45),c(-111,45), c(-111,44.5),c(-113,44.5),c(-114,45.6),c(-114.4,45.5), c(-114.4,46.5),c(-116,48),c(-116,49)))) sfcWestStates &lt;- st_sfc(CO,WY,UT,AZ,NM,CA,NV,OR,WA,ID,MT, crs=4326) attributes &lt;- bind_rows(c(name=&quot;Colorado&quot;, abb=&quot;CO&quot;, area=269837, pop=5758736), c(name=&quot;Wyoming&quot;, abb=&quot;WY&quot;, area=253600, pop=578759), c(name=&quot;Utah&quot;, abb=&quot;UT&quot;, area=84899, pop=3205958), c(name=&quot;Arizona&quot;, abb=&quot;AZ&quot;, area=295234, pop=7278717), c(name=&quot;New Mexico&quot;, abb=&quot;NM&quot;, area=314917, pop=2096829), c(name=&quot;California&quot;, abb=&quot;CA&quot;, area=423970, pop=39368078), c(name=&quot;Nevada&quot;, abb=&quot;NV&quot;, area=286382, pop=3080156), c(name=&quot;Oregon&quot;, abb=&quot;OR&quot;, area=254806,pop=4237256), c(name=&quot;Washington&quot;, abb=&quot;WA&quot;, area=184827,pop=7705281), c(name=&quot;Idaho&quot;, abb=&quot;ID&quot;, area=216443,pop=1839106), c(name=&quot;Montana&quot;, abb=&quot;MT&quot;, area=380800,pop=1085407)) %&gt;% mutate(area=as.numeric(area),pop=as.numeric(pop)) W_States &lt;- st_sf(attributes, geometry = sfcWestStates) dirname &lt;- &quot;data&quot; if (!dir.exists(dirname)) { dir.create(dirname) } 5.10 Exercises: Spatial Data and Maps Exercise 5.1 Create a map of W_States using ggplot, filling polygons with pop, using a scale_fill_gradient setting with a high of \"#1a5276\" and a low of \"#d4e6f1\" (hex color values), and use the aes() setting of geom_sf_text() to put labels in the states from the name variable. Exercise 5.2 Store the W_States as a shape file in the data folder with st_write. (Look up how to use st_write with ?st_write – it’s pretty simple.) Note that this will fail if it already exists, so include the parameter delete_layer = TRUE. Exercise 5.3 Highest Peaks: Create a tibble for the highest peaks in the western states, with the following names, elevations in m, longitude and latitude, use st_as_sf to create an sf from it, and add them to that map. Then use st_write again to store these as “data/peaks.shp” again using delete_layer = TRUE: Wheeler Peak, 4011, -105.4, 36.5 Mt. Whitney, 4421, -118.2, 36.5 Boundary Peak, 4007, -118.35, 37.9 Kings Peak, 4120, -110.3, 40.8 Gannett Peak, 4209, -109, 43.2 Mt. Elbert, 4401, -106.4, 39.1 Humphreys Peak, 3852, -111.7, 35.4 Mt. Hood, 3428, -121.7, 45.4 Mt. Rainier, 4392, -121.8, 46.9 Borah Peak, 3859, -113.8, 44.1 Granite Peak, 3903, -109.8, 45.1 Note: the easiest way to do this is with the tribble function, starting with: peaks &lt;- tribble( ~peak, ~elev, ~longitude, ~latitude, &quot;Wheeler Peak&quot;, 4011, -105.4, 36.5, Exercise 5.4 California freeways. From the CA_counties and CAfreeways feature data in igisci, make a simple map in ggplot, with freeways colored red). Exercise 5.5 After adding the terra library, create a raster from the built-in volcano matrix of elevations from Auckland’s Maunga Whau Volcano, and use plot() to display it. We’d do more with that dataset, but we don’t know what the cell size is. Exercise 5.6 Western States tmap: Use tmap to create a map from the W_States (polygons) and peaksp (points) data we created earlier. Include a basemap using the maptiles package. Hints: you’ll want to use tm_text with text set to “peak” to label the points, along with the parameter auto.placement=TRUE. Use this as an opportunity to test the shape files you’ve written earlier by using st_read with those shape files. Experiment with tm_symbol sizes, and just, xmod, and ymod settings with tm_text. Exercise 5.7 tmap view mode. Also using the western states data, create a tmap in view mode, but don’t use the state borders since the basemap will have them. Just before adding shapes, set the basemap to leaflet::providers\\$Esri.NatGeoWorldMapEsri.NatGeoWorldMap, then continue to the peaks after the + to see the peaks on a National Geographic basemap (Figure 5.20) FIGURE 5.20: tmap View mode (goal) References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
